{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_md\n",
    "from process import parse_sentence\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, BertModel, GPT2Model\n",
    "from mapper import Map, deduplication\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils import compress_attention, create_mapping, BFS, build_graph, is_word\n",
    "from multiprocessing import Pool\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertModel, GPT2Model\n",
    "from constant import invalid_relations_set\n",
    "from utils import compress_attention,build_graph\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:0\n",
      "sent:The invention relates to a roadable aircraft vehicle and related systems.\n",
      "sent start :0 sent end :12\n",
      "sent:An example roadable aircraft vehicle includes a vehicle drive system including an engine and gearbox selectively engageable with an automotive driveline and at least one propeller , a user interface including a display for controlling the drive system in an automotive mode including a steering wheel and in a flight mode including a control stick , a control system for switching between the flight mode and the automotive mode, and a system for locking the propeller during the automotive mode.\n",
      "sent start :12 sent end :94\n",
      "sent:The invention also relates to aircraft systems and elements such as an airfoil having a nominal profile, a folding wing , and an occupant crash protection system for an aircraft\n",
      "sent start :94 sent end :125\n",
      "{0: 'The invention', 2: 'a roadable aircraft vehicle', 4: 'related systems', 9: 'An example roadable aircraft vehicle', 12: 'a vehicle drive system', 18: 'an engine', 23: 'gearbox', 26: 'an automotive driveline', 28: 'at least one propeller', 30: 'a user interface', 34: 'a display', 39: 'the drive system', 43: 'an automotive mode', 47: 'a steering wheel', 51: 'a flight mode', 55: 'a control stick', 60: 'a control system', 64: 'the flight mode', 68: 'the automotive mode', 72: 'a system', 74: 'the propeller', 78: 'the automotive mode', 83: 'The invention', 87: 'aircraft systems', 90: 'elements', 94: 'an airfoil', 97: 'a nominal profile', 99: 'a folding wing', 102: 'an occupant crash protection system', 105: 'an aircraft', 108: 'relates to', 112: 'engageable with', 117: 'switching between', 123: 'relates to'}\n",
      "start_chunk: [0, 2, 4, 9, 12, 18, 23, 26, 28, 30, 34, 39, 43, 47, 51, 55, 60, 64, 68, 72, 74, 78, 83, 87, 90, 94, 97, 99, 102, 105, 108, 112, 117, 123], end_chunk: [2, 2, 4, 4, 8, 8, 11, 11, 17, 17, 22, 22, 25, 25, 27, 27, 30, 30, 33, 33, 38, 38, 42, 42, 45, 45, 50, 50, 54, 54, 58, 58, 63, 63, 67, 67, 71, 71, 74, 74, 77, 77, 81, 81, 85, 85, 89, 89, 93, 93, 96, 96, 99, 99, 101, 101, 103, 103, 107, 107, 111, 111, 115, 115, 122, 122, 125, 125]\n",
      "{0: 'The invention', 2: 'a roadable aircraft vehicle', 4: 'related systems', 9: 'An example roadable aircraft vehicle', 12: 'a vehicle drive system', 18: 'an engine', 23: 'gearbox', 26: 'an automotive driveline', 28: 'at least one propeller', 30: 'a user interface', 34: 'a display', 39: 'the drive system', 43: 'an automotive mode', 47: 'a steering wheel', 51: 'a flight mode', 55: 'a control stick', 60: 'a control system', 64: 'the flight mode', 68: 'the automotive mode', 72: 'a system', 74: 'the propeller', 78: 'the automotive mode', 83: 'The invention', 87: 'aircraft systems', 90: 'elements', 94: 'an airfoil', 97: 'a nominal profile', 99: 'a folding wing', 102: 'an occupant crash protection system', 105: 'an aircraft', 108: 'relates to', 112: 'engageable with', 117: 'switching between', 123: 'relates to'}\n",
      "sentence_mapping:['The invention', 'a roadable aircraft vehicle', 'related systems', 'and', 'An example roadable aircraft vehicle', '.', 'a vehicle drive system', 'includes', 'an engine', 'including', 'gearbox', 'and', 'an automotive driveline', 'selectively', 'at least one propeller', 'a user interface', 'and', 'a display', ',', 'the drive system', 'including', 'an automotive mode', 'for', 'controlling', 'a steering wheel', 'in', 'a flight mode', 'including', 'a control stick', 'and', 'in', 'a control system', 'including', 'the flight mode', ',', 'the automotive mode', 'for', 'a system', 'the propeller', 'and', 'the automotive mode', ',', 'and', 'The invention', 'for', 'locking', 'aircraft systems', 'during', 'elements', '.', 'an airfoil', 'also', 'a nominal profile', 'a folding wing', 'and', 'an occupant crash protection system', 'such', 'as', 'an aircraft', 'having', 'relates to', ',', 'engageable with', ',', 'and', 'switching between', 'for', 'relates to']\n",
      "noun_chunks:['The invention', 'a roadable aircraft vehicle', 'related systems', 'An example roadable aircraft vehicle', 'a vehicle drive system', 'an engine', 'gearbox', 'an automotive driveline', 'at least one propeller', 'a user interface', 'a display', 'the drive system', 'an automotive mode', 'a steering wheel', 'a flight mode', 'a control stick', 'a control system', 'the flight mode', 'the automotive mode', 'a system', 'the propeller', 'the automotive mode', 'The invention', 'aircraft systems', 'elements', 'an airfoil', 'a nominal profile', 'a folding wing', 'an occupant crash protection system', 'an aircraft']\n",
      "start_chunk:[0, 2, 4, 9, 12, 18, 23, 26, 28, 30, 34, 39, 43, 47, 51, 55, 60, 64, 68, 72, 74, 78, 83, 87, 90, 94, 97, 99, 102, 105, 108, 112, 117, 123]\n",
      "end_chunk:[2, 2, 4, 4, 8, 8, 11, 11, 17, 17, 22, 22, 25, 25, 27, 27, 30, 30, 33, 33, 38, 38, 42, 42, 45, 45, 50, 50, 54, 54, 58, 58, 63, 63, 67, 67, 71, 71, 74, 74, 77, 77, 81, 81, 85, 85, 89, 89, 93, 93, 96, 96, 99, 99, 101, 101, 103, 103, 107, 107, 111, 111, 115, 115, 122, 122, 125, 125]\n",
      "token2id:{'The invention': 43, 'a roadable aircraft vehicle': 1, 'related systems': 2, 'and': 64, 'An example roadable aircraft vehicle': 4, '.': 49, 'a vehicle drive system': 6, 'includes': 7, 'an engine': 8, 'including': 32, 'gearbox': 10, 'an automotive driveline': 12, 'selectively': 13, 'at least one propeller': 14, 'a user interface': 15, 'a display': 17, ',': 63, 'the drive system': 19, 'an automotive mode': 21, 'for': 66, 'controlling': 23, 'a steering wheel': 24, 'in': 30, 'a flight mode': 26, 'a control stick': 28, 'a control system': 31, 'the flight mode': 33, 'the automotive mode': 40, 'a system': 37, 'the propeller': 38, 'locking': 45, 'aircraft systems': 46, 'during': 47, 'elements': 48, 'an airfoil': 50, 'also': 51, 'a nominal profile': 52, 'a folding wing': 53, 'an occupant crash protection system': 55, 'such': 56, 'as': 57, 'an aircraft': 58, 'having': 59, 'relates to': 67, 'engageable with': 62, 'switching between': 65}\n",
      "token_ids:[102, 111, 28364, 103, 102, 106, 6229, 318, 12118, 4818, 103, 102, 1482, 1078, 103, 102, 137, 103, 102, 130, 1143, 6229, 318, 12118, 4818, 103, 102, 205, 103, 102, 106, 4818, 7021, 429, 103, 102, 3753, 103, 102, 130, 2393, 103, 102, 1471, 103, 102, 20603, 6119, 103, 102, 137, 103, 102, 130, 23163, 7021, 1169, 103, 102, 12531, 103, 102, 235, 1823, 482, 2940, 7799, 103, 102, 106, 1824, 3396, 103, 102, 137, 103, 102, 106, 3355, 103, 102, 422, 103, 102, 111, 7021, 429, 103, 102, 1471, 103, 102, 130, 23163, 2955, 103, 102, 168, 103, 102, 6702, 103, 102, 106, 17165, 12720, 103, 102, 121, 103, 102, 106, 10047, 2955, 103, 102, 1471, 103, 102, 106, 602, 20361, 103, 102, 137, 103, 102, 121, 103, 102, 106, 602, 429, 103, 102, 1471, 103, 102, 111, 10047, 2955, 103, 102, 422, 103, 102, 111, 23163, 2955, 103, 102, 168, 103, 102, 106, 429, 103, 102, 111, 2940, 7799, 103, 102, 137, 103, 102, 111, 23163, 2955, 103, 102, 422, 103, 102, 137, 103, 102, 111, 28364, 103, 102, 168, 103, 102, 23076, 103, 102, 12118, 1078, 103, 102, 781, 103, 102, 2419, 103, 102, 205, 103, 102, 130, 2385, 12006, 174, 103, 102, 469, 103, 102, 106, 9824, 3822, 103, 102, 106, 13019, 12688, 103, 102, 137, 103, 102, 130, 4529, 268, 20116, 4658, 429, 103, 102, 555, 103, 102, 188, 103, 102, 130, 12118, 103, 102, 2773, 103, 102, 13385, 147, 103, 102, 422, 103, 102, 11569, 318, 190, 103, 102, 422, 103, 102, 137, 103, 102, 5980, 467, 103, 102, 168, 103, 102, 13385, 147, 103]\n",
      "tokenid2word_mapping:[43, 43, 43, 43, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 64, 64, 64, 4, 4, 4, 4, 4, 4, 4, 4, 49, 49, 49, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 8, 8, 32, 32, 32, 10, 10, 10, 10, 64, 64, 64, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 64, 64, 64, 17, 17, 17, 17, 63, 63, 63, 19, 19, 19, 19, 19, 32, 32, 32, 21, 21, 21, 21, 21, 66, 66, 66, 23, 23, 23, 24, 24, 24, 24, 24, 30, 30, 30, 26, 26, 26, 26, 26, 32, 32, 32, 28, 28, 28, 28, 28, 64, 64, 64, 30, 30, 30, 31, 31, 31, 31, 31, 32, 32, 32, 33, 33, 33, 33, 33, 63, 63, 63, 40, 40, 40, 40, 40, 66, 66, 66, 37, 37, 37, 37, 38, 38, 38, 38, 38, 64, 64, 64, 40, 40, 40, 40, 40, 63, 63, 63, 64, 64, 64, 43, 43, 43, 43, 66, 66, 66, 45, 45, 45, 46, 46, 46, 46, 47, 47, 47, 48, 48, 48, 49, 49, 49, 50, 50, 50, 50, 50, 50, 51, 51, 51, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 64, 64, 64, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 57, 57, 57, 58, 58, 58, 58, 59, 59, 59, 67, 67, 67, 67, 63, 63, 63, 62, 62, 62, 62, 62, 63, 63, 63, 64, 64, 64, 65, 65, 65, 65, 66, 66, 66, 67, 67, 67, 67]\n",
      "subtoken_ids:[102, 13385, 147, 103]\n",
      "outputs:{'input_ids': tensor([[  102,   102,   111, 28364,   103,   102,   106,  6229,   318, 12118,\n",
      "          4818,   103,   102,  1482,  1078,   103,   102,   137,   103,   102,\n",
      "           130,  1143,  6229,   318, 12118,  4818,   103,   102,   205,   103,\n",
      "           102,   106,  4818,  7021,   429,   103,   102,  3753,   103,   102,\n",
      "           130,  2393,   103,   102,  1471,   103,   102, 20603,  6119,   103,\n",
      "           102,   137,   103,   102,   130, 23163,  7021,  1169,   103,   102,\n",
      "         12531,   103,   102,   235,  1823,   482,  2940,  7799,   103,   102,\n",
      "           106,  1824,  3396,   103,   102,   137,   103,   102,   106,  3355,\n",
      "           103,   102,   422,   103,   102,   111,  7021,   429,   103,   102,\n",
      "          1471,   103,   102,   130, 23163,  2955,   103,   102,   168,   103,\n",
      "           102,  6702,   103,   102,   106, 17165, 12720,   103,   102,   121,\n",
      "           103,   102,   106, 10047,  2955,   103,   102,  1471,   103,   102,\n",
      "           106,   602, 20361,   103,   102,   137,   103,   102,   121,   103,\n",
      "           102,   106,   602,   429,   103,   102,  1471,   103,   102,   111,\n",
      "         10047,  2955,   103,   102,   422,   103,   102,   111, 23163,  2955,\n",
      "           103,   102,   168,   103,   102,   106,   429,   103,   102,   111,\n",
      "          2940,  7799,   103,   102,   137,   103,   102,   111, 23163,  2955,\n",
      "           103,   102,   422,   103,   102,   137,   103,   102,   111, 28364,\n",
      "           103,   102,   168,   103,   102, 23076,   103,   102, 12118,  1078,\n",
      "           103,   102,   781,   103,   102,  2419,   103,   102,   205,   103,\n",
      "           102,   130,  2385, 12006,   174,   103,   102,   469,   103,   102,\n",
      "           106,  9824,  3822,   103,   102,   106, 13019, 12688,   103,   102,\n",
      "           137,   103,   102,   130,  4529,   268, 20116,  4658,   429,   103,\n",
      "           102,   555,   103,   102,   188,   103,   102,   130, 12118,   103,\n",
      "           102,  2773,   103,   102, 13385,   147,   103,   102,   422,   103,\n",
      "           102, 11569,   318,   190,   103,   102,   422,   103,   102,   137,\n",
      "           103,   102,  5980,   467,   103,   102,   168,   103,   102, 13385,\n",
      "           147,   103,   103]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "len(tokenids):271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn:tensor([[[1.3194e-03, 5.2866e-03, 4.8729e-02,  ..., 2.4504e-03,\n",
      "          1.0390e-03, 1.0441e-03],\n",
      "         [6.0316e-06, 9.1629e-04, 2.4702e-02,  ..., 7.0470e-03,\n",
      "          8.1012e-03, 7.8630e-03],\n",
      "         [1.1550e-03, 6.8930e-03, 4.8454e-03,  ..., 1.6150e-03,\n",
      "          5.7780e-03, 5.8672e-03],\n",
      "         ...,\n",
      "         [8.2698e-03, 5.2186e-03, 1.1070e-03,  ..., 3.2867e-03,\n",
      "          4.8564e-03, 4.7310e-03],\n",
      "         [2.7200e-02, 1.3011e-02, 2.6921e-03,  ..., 5.0922e-04,\n",
      "          2.0170e-03, 1.9465e-03],\n",
      "         [2.5338e-02, 1.2095e-02, 2.6254e-03,  ..., 5.5666e-04,\n",
      "          2.3277e-03, 2.2457e-03]],\n",
      "\n",
      "        [[3.8701e-04, 1.2004e-03, 2.6828e-05,  ..., 2.3535e-05,\n",
      "          1.3305e-02, 1.0456e-02],\n",
      "         [3.7791e-04, 3.7617e-02, 1.6443e-04,  ..., 7.0442e-06,\n",
      "          6.1450e-05, 5.2941e-05],\n",
      "         [1.6356e-03, 1.1078e-02, 2.1066e-03,  ..., 6.9912e-04,\n",
      "          2.6600e-03, 2.5690e-03],\n",
      "         ...,\n",
      "         [1.0210e-03, 6.4660e-04, 7.3620e-04,  ..., 2.1362e-03,\n",
      "          1.1360e-02, 1.1612e-02],\n",
      "         [1.0896e-03, 2.9192e-03, 1.5816e-04,  ..., 7.6589e-04,\n",
      "          1.6154e-02, 1.6030e-02],\n",
      "         [9.6122e-04, 2.8437e-03, 1.4168e-04,  ..., 6.4817e-04,\n",
      "          1.6729e-02, 1.4692e-02]],\n",
      "\n",
      "        [[1.3414e-03, 8.6112e-03, 1.5467e-03,  ..., 5.9145e-04,\n",
      "          1.6737e-03, 1.6093e-03],\n",
      "         [3.8744e-04, 2.4975e-03, 1.0651e-03,  ..., 4.8565e-04,\n",
      "          1.2370e-03, 1.1480e-03],\n",
      "         [1.0463e-03, 4.9802e-03, 8.3131e-04,  ..., 9.4446e-04,\n",
      "          3.4073e-03, 2.6719e-03],\n",
      "         ...,\n",
      "         [1.9283e-03, 8.5797e-03, 2.7116e-03,  ..., 6.8272e-04,\n",
      "          1.6793e-03, 2.9585e-03],\n",
      "         [4.6875e-03, 9.8685e-03, 2.7296e-03,  ..., 3.8033e-04,\n",
      "          1.2098e-03, 1.8370e-03],\n",
      "         [4.5003e-03, 9.5340e-03, 2.3747e-03,  ..., 7.7153e-04,\n",
      "          1.9425e-03, 1.0944e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.3142e-02, 1.5273e-02, 1.8260e-03,  ..., 1.4224e-03,\n",
      "          3.7979e-04, 3.7665e-04],\n",
      "         [4.0523e-03, 1.7137e-02, 4.2729e-03,  ..., 1.1835e-03,\n",
      "          5.7983e-04, 5.5892e-04],\n",
      "         [4.2046e-03, 1.0999e-02, 1.3852e-03,  ..., 1.2436e-03,\n",
      "          9.5986e-03, 8.5396e-03],\n",
      "         ...,\n",
      "         [2.2608e-03, 3.5227e-03, 4.7197e-04,  ..., 8.2849e-04,\n",
      "          2.1095e-02, 1.9386e-02],\n",
      "         [1.5422e-02, 1.4800e-02, 1.1094e-03,  ..., 1.7899e-03,\n",
      "          2.8082e-03, 2.8481e-03],\n",
      "         [1.8449e-02, 1.6061e-02, 8.6608e-04,  ..., 1.8188e-03,\n",
      "          2.3255e-03, 2.2685e-03]],\n",
      "\n",
      "        [[1.4715e-02, 1.8059e-02, 1.9098e-04,  ..., 5.4568e-04,\n",
      "          2.2751e-04, 2.0671e-04],\n",
      "         [1.2861e-03, 2.9533e-02, 1.2309e-03,  ..., 4.4392e-04,\n",
      "          2.3861e-04, 2.0804e-04],\n",
      "         [5.1990e-04, 2.5960e-03, 1.3360e-03,  ..., 1.0189e-03,\n",
      "          2.3786e-03, 2.1653e-03],\n",
      "         ...,\n",
      "         [1.1682e-03, 8.9128e-04, 6.8590e-04,  ..., 9.1229e-04,\n",
      "          4.1113e-03, 4.8097e-03],\n",
      "         [7.0414e-03, 1.5051e-02, 1.3754e-04,  ..., 2.9621e-04,\n",
      "          2.3099e-04, 2.3475e-04],\n",
      "         [7.0168e-03, 1.4192e-02, 1.1315e-04,  ..., 3.3220e-04,\n",
      "          2.1330e-04, 2.0475e-04]],\n",
      "\n",
      "        [[1.8058e-02, 6.4299e-03, 7.1270e-04,  ..., 8.2737e-04,\n",
      "          2.7955e-03, 2.8366e-03],\n",
      "         [1.6811e-03, 2.2190e-01, 1.9203e-02,  ..., 3.5087e-05,\n",
      "          2.8311e-05, 3.1890e-05],\n",
      "         [1.2023e-03, 6.0639e-01, 1.2590e-02,  ..., 9.2690e-06,\n",
      "          3.7100e-06, 2.7887e-06],\n",
      "         ...,\n",
      "         [7.6727e-05, 8.4507e-05, 3.0736e-05,  ..., 6.8263e-03,\n",
      "          8.9380e-02, 3.3632e-02],\n",
      "         [5.7231e-03, 1.4705e-04, 7.6864e-08,  ..., 1.9009e-03,\n",
      "          2.5017e-03, 1.1836e-03],\n",
      "         [8.3518e-03, 1.4254e-04, 3.7151e-08,  ..., 1.2150e-03,\n",
      "          4.2632e-03, 4.2255e-03]]])\n",
      "(271, 271)\n",
      "merged_attention:[[0.03304378 0.01889454 0.01057447 ... 0.00205523 0.0021971  0.00162563]\n",
      " [0.01650373 0.02326584 0.02360877 ... 0.00198469 0.00173793 0.00145129]\n",
      " [0.01050963 0.0121934  0.03239858 ... 0.00276855 0.00226568 0.00191512]\n",
      " ...\n",
      " [0.00391742 0.00295684 0.00399054 ... 0.03289765 0.02500092 0.00803548]\n",
      " [0.00414931 0.00226703 0.00359781 ... 0.0143082  0.03952025 0.02410345]\n",
      " [0.00370666 0.00215079 0.003444   ... 0.00755372 0.01804109 0.04047835]],shape:(68, 68)\n",
      "attn_graph:defaultdict(<class 'list'>, {0: [(1, 0.018894536), (2, 0.0105744675), (3, 0.009990688), (4, 0.0053998902), (5, 0.007030284), (6, 0.0043022786), (7, 0.0064714053), (8, 0.005447858), (9, 0.0059304615), (10, 0.0041140188), (11, 0.0044861045), (12, 0.0028430664), (13, 0.003617375), (14, 0.0021260914), (15, 0.002900998), (16, 0.0038283507), (17, 0.0032186313), (18, 0.0034698194), (19, 0.0032024856), (20, 0.0033079814), (21, 0.0022245408), (22, 0.0028239023), (23, 0.0026876938), (24, 0.0020506282), (25, 0.002925208), (26, 0.0020073082), (27, 0.0024911545), (28, 0.0015407689), (29, 0.0017494243), (30, 0.0015603708), (31, 0.0012770395), (32, 0.0016954356), (33, 0.0016378447), (34, 0.0016223085), (35, 0.0016428239), (36, 0.0018508752), (37, 0.0018690112), (38, 0.0019144665), (39, 0.0018181292), (40, 0.0018364384), (41, 0.0015808049), (42, 0.0015517155), (43, 0.0023813979), (44, 0.0017718951), (45, 0.0017314058), (46, 0.001604181), (47, 0.0019038346), (48, 0.0020417338), (49, 0.002186634), (50, 0.0015435527), (51, 0.0025470275), (52, 0.0020335265), (53, 0.001972949), (54, 0.0024616024), (55, 0.001543706), (56, 0.0027253532), (57, 0.0026244076), (58, 0.002508994), (59, 0.0026010582), (60, 0.0025086962), (61, 0.0029243107), (62, 0.002176496), (63, 0.0025633855), (64, 0.002421313), (65, 0.0020552296), (66, 0.002197105), (67, 0.0016256256)], 1: [(2, 0.023608768), (3, 0.01312456), (4, 0.007754524), (5, 0.006829334), (6, 0.005130832), (7, 0.0055063465), (8, 0.004592806), (9, 0.0047451165), (10, 0.004206077), (11, 0.0038498032), (12, 0.0033954896), (13, 0.0032468038), (14, 0.0019059984), (15, 0.0025532464), (16, 0.0029761016), (17, 0.0027077352), (18, 0.0026789175), (19, 0.0024675091), (20, 0.0024177544), (21, 0.0024371832), (22, 0.002058326), (23, 0.002096383), (24, 0.0020975336), (25, 0.0022659951), (26, 0.0020802051), (27, 0.0020455278), (28, 0.0014442168), (29, 0.0014251824), (30, 0.0012702906), (31, 0.0010630047), (32, 0.001488879), (33, 0.001612358), (34, 0.0014942315), (35, 0.0018677434), (36, 0.0015556443), (37, 0.0015222691), (38, 0.0018184433), (39, 0.0017732475), (40, 0.0019773429), (41, 0.0013809917), (42, 0.0014440411), (43, 0.0016730366), (44, 0.0015569323), (45, 0.0017400705), (46, 0.0023009132), (47, 0.0018685544), (48, 0.0018514049), (49, 0.0018451713), (50, 0.0015281793), (51, 0.0021863745), (52, 0.0018790305), (53, 0.0020292373), (54, 0.002305257), (55, 0.0019239892), (56, 0.0024137886), (57, 0.002431239), (58, 0.0026450728), (59, 0.0020424477), (60, 0.0019047345), (61, 0.0023542915), (62, 0.002493381), (63, 0.0022127647), (64, 0.0021878155), (65, 0.001984693), (66, 0.0017379328), (67, 0.0014512853)], 2: [(3, 0.030284671), (4, 0.008240347), (5, 0.007745989), (6, 0.005716289), (7, 0.007483116), (8, 0.0048570232), (9, 0.0050748433), (10, 0.0041897977), (11, 0.003789164), (12, 0.0031789497), (13, 0.003608223), (14, 0.001800129), (15, 0.0028301557), (16, 0.0028360581), (17, 0.0027316017), (18, 0.0029286535), (19, 0.0031868846), (20, 0.0025270565), (21, 0.0021777884), (22, 0.0019270008), (23, 0.0019969975), (24, 0.0016463983), (25, 0.0019260937), (26, 0.0016532069), (27, 0.001750588), (28, 0.001394208), (29, 0.0013425556), (30, 0.0012420093), (31, 0.0011345204), (32, 0.0014064489), (33, 0.0016342215), (34, 0.0015552334), (35, 0.0018213404), (36, 0.0014845167), (37, 0.001691631), (38, 0.0018159192), (39, 0.0017576021), (40, 0.0020929435), (41, 0.0013438355), (42, 0.0013707625), (43, 0.0018674367), (44, 0.0016245422), (45, 0.0018419997), (46, 0.0020604322), (47, 0.0017644238), (48, 0.0018436052), (49, 0.0017843306), (50, 0.0013095808), (51, 0.001957328), (52, 0.0017518227), (53, 0.0016859518), (54, 0.0020652593), (55, 0.0018683875), (56, 0.0023013263), (57, 0.002466542), (58, 0.002694034), (59, 0.0020435085), (60, 0.002079977), (61, 0.0021339722), (62, 0.0020521726), (63, 0.0022792288), (64, 0.0027296813), (65, 0.0027685503), (66, 0.002265684), (67, 0.0019151247)], 3: [(4, 0.013740915), (5, 0.010314105), (6, 0.0054500825), (7, 0.008748684), (8, 0.0060045198), (9, 0.005889093), (10, 0.0038887348), (11, 0.004601298), (12, 0.0027303603), (13, 0.0036607636), (14, 0.001978081), (15, 0.002465185), (16, 0.0035880748), (17, 0.002986466), (18, 0.003941217), (19, 0.0032319608), (20, 0.0029655723), (21, 0.0019192987), (22, 0.002228265), (23, 0.0020552494), (24, 0.0014474018), (25, 0.0022763896), (26, 0.0015475728), (27, 0.0021419206), (28, 0.0012653837), (29, 0.0015602136), (30, 0.0012660129), (31, 0.001113986), (32, 0.0015067003), (33, 0.0016477036), (34, 0.001629155), (35, 0.0015820604), (36, 0.0016328368), (37, 0.0015422902), (38, 0.0019213684), (39, 0.0019245386), (40, 0.0017913472), (41, 0.0016913433), (42, 0.0016190213), (43, 0.0018410984), (44, 0.0017777812), (45, 0.001565137), (46, 0.00146903), (47, 0.0019271198), (48, 0.0018708757), (49, 0.002015593), (50, 0.0013386192), (51, 0.0021156992), (52, 0.0015299711), (53, 0.0014564345), (54, 0.0021666079), (55, 0.0011719202), (56, 0.002447963), (57, 0.0027230643), (58, 0.0021504583), (59, 0.0022592577), (60, 0.0020417303), (61, 0.0027155469), (62, 0.0018049751), (63, 0.0025241051), (64, 0.0025558819), (65, 0.003182742), (66, 0.0027472198), (67, 0.002185243)], 4: [(5, 0.025839528), (6, 0.009478201), (7, 0.008904276), (8, 0.006688859), (9, 0.0064552277), (10, 0.0048327437), (11, 0.0037996864), (12, 0.003464646), (13, 0.0037229785), (14, 0.002134192), (15, 0.0025954074), (16, 0.0028234746), (17, 0.002765026), (18, 0.0028952707), (19, 0.002761242), (20, 0.0030142413), (21, 0.0025811642), (22, 0.0020516475), (23, 0.001868255), (24, 0.0017322481), (25, 0.0016554571), (26, 0.0015074819), (27, 0.001552984), (28, 0.0011710528), (29, 0.0012567014), (30, 0.0011045077), (31, 0.00095347), (32, 0.0013897279), (33, 0.0014532217), (34, 0.0013510672), (35, 0.0016321603), (36, 0.0013876898), (37, 0.0012780855), (38, 0.0014850034), (39, 0.0016191181), (40, 0.0019275009), (41, 0.0014518605), (42, 0.0016159844), (43, 0.0018688734), (44, 0.0018775441), (45, 0.0019922804), (46, 0.002100644), (47, 0.0018908675), (48, 0.0017990401), (49, 0.0017414922), (50, 0.00132494), (51, 0.0017610543), (52, 0.001465719), (53, 0.0015986345), (54, 0.0018370684), (55, 0.0014648258), (56, 0.0019317215), (57, 0.0020592634), (58, 0.0024239072), (59, 0.0020872734), (60, 0.0019252951), (61, 0.0020853288), (62, 0.00249082), (63, 0.00190681), (64, 0.0020087752), (65, 0.0020323694), (66, 0.0023648674), (67, 0.0023198)], 5: [(6, 0.019149182), (7, 0.0122478055), (8, 0.0073338905), (9, 0.007865532), (10, 0.0054706046), (11, 0.005862169), (12, 0.0030140837), (13, 0.003912932), (14, 0.0020188033), (15, 0.0025484231), (16, 0.0035656446), (17, 0.0032675024), (18, 0.003990621), (19, 0.0035166666), (20, 0.0034451953), (21, 0.0023429554), (22, 0.002735093), (23, 0.0022397046), (24, 0.0015142781), (25, 0.0020218028), (26, 0.0014152734), (27, 0.0016528657), (28, 0.0010992133), (29, 0.0011724167), (30, 0.0012449827), (31, 0.0009275888), (32, 0.0015478949), (33, 0.0015601933), (34, 0.0017158788), (35, 0.0014382896), (36, 0.0014759437), (37, 0.0013198778), (38, 0.0017839379), (39, 0.0019398298), (40, 0.0017977158), (41, 0.0018365174), (42, 0.0018986206), (43, 0.0021290244), (44, 0.002125653), (45, 0.0020277037), (46, 0.0017550645), (47, 0.0024924895), (48, 0.002137238), (49, 0.0025760473), (50, 0.0013833832), (51, 0.0020972325), (52, 0.0015140222), (53, 0.0014007607), (54, 0.0021281785), (55, 0.00095774536), (56, 0.0021125034), (57, 0.0024394616), (58, 0.0017854697), (59, 0.0020035582), (60, 0.0019065695), (61, 0.0022027304), (62, 0.0015601626), (63, 0.0021215302), (64, 0.0019863632), (65, 0.0016888415), (66, 0.0020219989), (67, 0.0018653818)], 6: [(7, 0.029418474), (8, 0.01011749), (9, 0.0070114746), (10, 0.0062920325), (11, 0.005177113), (12, 0.0055401255), (13, 0.0039440086), (14, 0.002080619), (15, 0.002926653), (16, 0.0026582412), (17, 0.0029371455), (18, 0.0029756923), (19, 0.0031573772), (20, 0.0028106452), (21, 0.0032511014), (22, 0.0023296976), (23, 0.0023099768), (24, 0.0021546402), (25, 0.0018285286), (26, 0.0016207754), (27, 0.0015530144), (28, 0.0011159714), (29, 0.0011722046), (30, 0.00103562), (31, 0.0011353462), (32, 0.0011384977), (33, 0.0014242518), (34, 0.0012038042), (35, 0.0018442475), (36, 0.001254796), (37, 0.0014643194), (38, 0.0015342722), (39, 0.0016112769), (40, 0.002422684), (41, 0.0015802797), (42, 0.001641014), (43, 0.0021647294), (44, 0.0019489975), (45, 0.002349741), (46, 0.0030806183), (47, 0.0026929246), (48, 0.0030549734), (49, 0.0024113997), (50, 0.0016535876), (51, 0.0019820782), (52, 0.0017036593), (53, 0.0017808626), (54, 0.0017101149), (55, 0.0017229567), (56, 0.0015491284), (57, 0.0017109712), (58, 0.0021128866), (59, 0.0015834459), (60, 0.0013850924), (61, 0.0015006149), (62, 0.0013871645), (63, 0.0014156363), (64, 0.001395251), (65, 0.001589611), (66, 0.0015764134), (67, 0.0014705068)], 7: [(8, 0.025814997), (9, 0.0119562), (10, 0.00678557), (11, 0.0068148295), (12, 0.004104721), (13, 0.0053034476), (14, 0.0023281812), (15, 0.0024201772), (16, 0.0031346781), (17, 0.0028884006), (18, 0.0035885833), (19, 0.0035234347), (20, 0.0034950932), (21, 0.0025433053), (22, 0.0029806711), (23, 0.0029147693), (24, 0.0017475467), (25, 0.0023493648), (26, 0.0015375408), (27, 0.0022743128), (28, 0.0011196667), (29, 0.0012899637), (30, 0.0012611115), (31, 0.00097400526), (32, 0.0014365227), (33, 0.0013997981), (34, 0.0013973623), (35, 0.001461245), (36, 0.0015621613), (37, 0.0013131032), (38, 0.0018768653), (39, 0.001859632), (40, 0.0021183859), (41, 0.0020004741), (42, 0.0020648402), (43, 0.0027742018), (44, 0.0023889972), (45, 0.002542503), (46, 0.0022740713), (47, 0.0032656786), (48, 0.0037177075), (49, 0.0043121562), (50, 0.002193609), (51, 0.0030044506), (52, 0.0021520334), (53, 0.0017453169), (54, 0.002243853), (55, 0.0011653827), (56, 0.001792184), (57, 0.0020848087), (58, 0.0016147583), (59, 0.0019756665), (60, 0.0016127983), (61, 0.0017055761), (62, 0.001269895), (63, 0.0016389308), (64, 0.0013899896), (65, 0.0013536189), (66, 0.0015606355), (67, 0.0015589466)], 8: [(9, 0.029675124), (10, 0.008631225), (11, 0.0072950968), (12, 0.0053440924), (13, 0.006136688), (14, 0.0029478886), (15, 0.0029372722), (16, 0.0032077248), (17, 0.0027629868), (18, 0.0032138198), (19, 0.0031859553), (20, 0.0035715576), (21, 0.0028734058), (22, 0.002901478), (23, 0.0025019504), (24, 0.0019913125), (25, 0.0022052217), (26, 0.0019443609), (27, 0.0019374625), (28, 0.0012333917), (29, 0.0012358639), (30, 0.001160969), (31, 0.00091552036), (32, 0.0013671933), (33, 0.0014671381), (34, 0.001207239), (35, 0.001485228), (36, 0.0012814979), (37, 0.0012562391), (38, 0.0016315958), (39, 0.0018083863), (40, 0.0021249424), (41, 0.0016668107), (42, 0.0018993942), (43, 0.0026195315), (44, 0.0022856626), (45, 0.0021769113), (46, 0.0026752583), (47, 0.0027105305), (48, 0.002856796), (49, 0.003774142), (50, 0.0030333793), (51, 0.0030626298), (52, 0.0021698643), (53, 0.002007192), (54, 0.0024590034), (55, 0.0013292718), (56, 0.0021290442), (57, 0.0020558026), (58, 0.002407895), (59, 0.0017422329), (60, 0.001576143), (61, 0.001640509), (62, 0.0013482686), (63, 0.0014895387), (64, 0.0015544803), (65, 0.0012641221), (66, 0.0015352665), (67, 0.0014267897)], 9: [(10, 0.023128908), (11, 0.010425673), (12, 0.005245453), (13, 0.0071972874), (14, 0.0034538987), (15, 0.0037012547), (16, 0.0037987756), (17, 0.0031383098), (18, 0.0034794714), (19, 0.0033737123), (20, 0.003692399), (21, 0.0027041591), (22, 0.003376798), (23, 0.0030184228), (24, 0.001923565), (25, 0.0025356251), (26, 0.0016576691), (27, 0.0028896306), (28, 0.0012853934), (29, 0.0013523669), (30, 0.0012967435), (31, 0.0009797622), (32, 0.0020900911), (33, 0.0014079055), (34, 0.0012439842), (35, 0.0013123746), (36, 0.0013651926), (37, 0.0011542155), (38, 0.0016115342), (39, 0.0016057714), (40, 0.0019369262), (41, 0.0018025633), (42, 0.0016758031), (43, 0.0022620787), (44, 0.0020790373), (45, 0.0020355512), (46, 0.001669527), (47, 0.0024441807), (48, 0.0028557368), (49, 0.002996533), (50, 0.0022350624), (51, 0.0037266724), (52, 0.0024184561), (53, 0.001992789), (54, 0.0029857282), (55, 0.0013873711), (56, 0.0021770766), (57, 0.0024669168), (58, 0.0019237606), (59, 0.002258717), (60, 0.0019066086), (61, 0.0018957626), (62, 0.0013494487), (63, 0.0017150262), (64, 0.0015083124), (65, 0.0014532127), (66, 0.0016539479), (67, 0.0016353941)], 10: [(11, 0.030143833), (12, 0.007829342), (13, 0.0071511194), (14, 0.003574163), (15, 0.004001672), (16, 0.005002453), (17, 0.0033581334), (18, 0.003502852), (19, 0.003198435), (20, 0.0029789081), (21, 0.0027916883), (22, 0.00332316), (23, 0.002837471), (24, 0.002561709), (25, 0.0025404643), (26, 0.001961414), (27, 0.0021157719), (28, 0.0014506404), (29, 0.0014005109), (30, 0.0013101529), (31, 0.0010010861), (32, 0.0014355909), (33, 0.0014951959), (34, 0.0013838244), (35, 0.0014869941), (36, 0.0012581759), (37, 0.0011247913), (38, 0.0014976131), (39, 0.0016589003), (40, 0.0018989418), (41, 0.0015810138), (42, 0.0017539505), (43, 0.0020950213), (44, 0.0018973788), (45, 0.0020546494), (46, 0.0019891271), (47, 0.0023271448), (48, 0.002566147), (49, 0.0026112085), (50, 0.0018607723), (51, 0.0039022546), (52, 0.0027834983), (53, 0.0025157365), (54, 0.002999109), (55, 0.0020861372), (56, 0.002394027), (57, 0.0024569493), (58, 0.0024455637), (59, 0.0021466997), (60, 0.0019023456), (61, 0.001989655), (62, 0.0016158328), (63, 0.0017981419), (64, 0.0017338562), (65, 0.0014596133), (66, 0.0016750777), (67, 0.0014478995)], 11: [(12, 0.016621536), (13, 0.009305533), (14, 0.004107255), (15, 0.0040735505), (16, 0.005927136), (17, 0.0043160086), (18, 0.0043570343), (19, 0.0037441843), (20, 0.0031566117), (21, 0.0024978528), (22, 0.0035396805), (23, 0.0031215316), (24, 0.0021605245), (25, 0.0030764807), (26, 0.0019600238), (27, 0.002612019), (28, 0.0015541473), (29, 0.0017529599), (30, 0.0014786866), (31, 0.0010844874), (32, 0.0014516567), (33, 0.0015822023), (34, 0.0015359331), (35, 0.0014758711), (36, 0.0014627804), (37, 0.001150254), (38, 0.0015251109), (39, 0.0016786313), (40, 0.0017977627), (41, 0.0016480624), (42, 0.0015985896), (43, 0.0020685773), (44, 0.0018256316), (45, 0.0016748033), (46, 0.0013897779), (47, 0.0020270515), (48, 0.0024036565), (49, 0.0024767958), (50, 0.0016701878), (51, 0.0030998823), (52, 0.002500298), (53, 0.0022745735), (54, 0.002950562), (55, 0.0015967493), (56, 0.0028544005), (57, 0.003200245), (58, 0.0023588373), (59, 0.00232476), (60, 0.0020478806), (61, 0.0024467993), (62, 0.0015195381), (63, 0.0019957158), (64, 0.0018330872), (65, 0.0020623214), (66, 0.00189058), (67, 0.0016492362)], 12: [(13, 0.02702487), (14, 0.0058005354), (15, 0.005178417), (16, 0.00504416), (17, 0.004351766), (18, 0.0049893567), (19, 0.004156889), (20, 0.0030697135), (21, 0.0031359172), (22, 0.002778844), (23, 0.0027079715), (24, 0.0026561378), (25, 0.002398672), (26, 0.002339222), (27, 0.0022324345), (28, 0.0016239533), (29, 0.0017110213), (30, 0.0015292531), (31, 0.0011702401), (32, 0.001446736), (33, 0.001613471), (34, 0.0012929396), (35, 0.002002866), (36, 0.0011857094), (37, 0.0010619464), (38, 0.0012127673), (39, 0.0013126781), (40, 0.0020305288), (41, 0.0013858968), (42, 0.0014485712), (43, 0.0018056154), (44, 0.0015000781), (45, 0.0016769115), (46, 0.0018943264), (47, 0.0014811676), (48, 0.0017798557), (49, 0.0018403718), (50, 0.001432367), (51, 0.0020912045), (52, 0.0020228918), (53, 0.003089131), (54, 0.00289482), (55, 0.002192242), (56, 0.0036117441), (57, 0.0039661443), (58, 0.0043329773), (59, 0.002737622), (60, 0.0022724273), (61, 0.002160019), (62, 0.0018767414), (63, 0.0019983693), (64, 0.0020136263), (65, 0.0020364441), (66, 0.0021591007), (67, 0.0019902042)], 13: [(14, 0.015626911), (15, 0.0062344214), (16, 0.007130482), (17, 0.005260146), (18, 0.0058744173), (19, 0.004977048), (20, 0.0042797765), (21, 0.0027609095), (22, 0.003643663), (23, 0.0034205765), (24, 0.0024548855), (25, 0.003165504), (26, 0.0021905946), (27, 0.0028343687), (28, 0.001871942), (29, 0.0018470647), (30, 0.0018973885), (31, 0.0014397328), (32, 0.0019001128), (33, 0.001785424), (34, 0.0015317597), (35, 0.001549441), (36, 0.0013210312), (37, 0.001185493), (38, 0.0014825918), (39, 0.0015755432), (40, 0.0016606839), (41, 0.0015493673), (42, 0.0016185092), (43, 0.0019139127), (44, 0.0016212534), (45, 0.0014982385), (46, 0.0014122679), (47, 0.0017122653), (48, 0.0016380515), (49, 0.0019070137), (50, 0.0011794482), (51, 0.0019407711), (52, 0.0017810601), (53, 0.0019798328), (54, 0.0027281425), (55, 0.0016192875), (56, 0.002566926), (57, 0.0029967085), (58, 0.0030861539), (59, 0.0029598987), (60, 0.0024292613), (61, 0.0024807877), (62, 0.0018547832), (63, 0.0022851492), (64, 0.0021810608), (65, 0.0018449047), (66, 0.0021076438), (67, 0.0018956853)], 14: [(15, 0.015468171), (16, 0.008572993), (17, 0.006001139), (18, 0.00559043), (19, 0.0044856034), (20, 0.0056369863), (21, 0.0038370162), (22, 0.0041245236), (23, 0.0039112163), (24, 0.002949842), (25, 0.0034155215), (26, 0.002431549), (27, 0.0031970155), (28, 0.0020774105), (29, 0.0022265085), (30, 0.0020423578), (31, 0.0016456491), (32, 0.0022579718), (33, 0.0018315341), (34, 0.0018090779), (35, 0.0017398922), (36, 0.0017264239), (37, 0.0013160635), (38, 0.0019665787), (39, 0.0017173815), (40, 0.001682821), (41, 0.0015628942), (42, 0.0016356279), (43, 0.0019791094), (44, 0.0017722809), (45, 0.0017277552), (46, 0.0015474488), (47, 0.0017313088), (48, 0.0017145291), (49, 0.001587988), (50, 0.0013369923), (51, 0.0019685244), (52, 0.0017624354), (53, 0.0019062646), (54, 0.0025067176), (55, 0.0017413306), (56, 0.0025615562), (57, 0.0027377605), (58, 0.003025731), (59, 0.0041798516), (60, 0.0036103332), (61, 0.0030215078), (62, 0.0023730858), (63, 0.002509393), (64, 0.0024206853), (65, 0.002368376), (66, 0.002526305), (67, 0.0024214312)], 15: [(16, 0.02925106), (17, 0.010764478), (18, 0.006665194), (19, 0.0056375545), (20, 0.0051768697), (21, 0.005173086), (22, 0.004758427), (23, 0.0042660455), (24, 0.0032601957), (25, 0.0033979577), (26, 0.0029561236), (27, 0.0031493332), (28, 0.0022371819), (29, 0.0022546086), (30, 0.0021497037), (31, 0.002138067), (32, 0.0021543761), (33, 0.0021239538), (34, 0.0017482218), (35, 0.001975113), (36, 0.00162304), (37, 0.001477435), (38, 0.0014523672), (39, 0.00148318), (40, 0.001724083), (41, 0.0015431464), (42, 0.0015486497), (43, 0.0017493436), (44, 0.001399773), (45, 0.0016990354), (46, 0.0015669831), (47, 0.0013318467), (48, 0.001514831), (49, 0.0013918261), (50, 0.0010037172), (51, 0.0015548301), (52, 0.0015177529), (53, 0.0016417138), (54, 0.0018612583), (55, 0.001754375), (56, 0.002050216), (57, 0.0021292374), (58, 0.002482528), (59, 0.0024697233), (60, 0.0023877136), (61, 0.0028751998), (62, 0.0028372016), (63, 0.0024834492), (64, 0.0023894934), (65, 0.0024381722), (66, 0.0022847662), (67, 0.0020303456)], 16: [(17, 0.024317939), (18, 0.01213405), (19, 0.007408145), (20, 0.006280973), (21, 0.004483643), (22, 0.0075988886), (23, 0.0053931084), (24, 0.003043465), (25, 0.0040102326), (26, 0.0026383253), (27, 0.0037021905), (28, 0.0021089786), (29, 0.0025035006), (30, 0.0024334965), (31, 0.0019606627), (32, 0.002855721), (33, 0.002465168), (34, 0.0025220064), (35, 0.0021883915), (36, 0.0022445107), (37, 0.0016068063), (38, 0.0018386033), (39, 0.0020448433), (40, 0.0018060555), (41, 0.0018384716), (42, 0.0016727676), (43, 0.0019528571), (44, 0.0017020502), (45, 0.0015318148), (46, 0.001245448), (47, 0.0016702386), (48, 0.001617314), (49, 0.0015442272), (50, 0.0010722843), (51, 0.0015169308), (52, 0.0011735146), (53, 0.0012496642), (54, 0.0018562485), (55, 0.0010424659), (56, 0.0019214734), (57, 0.002323457), (58, 0.0018542312), (59, 0.0020233502), (60, 0.002034661), (61, 0.002533312), (62, 0.0018432762), (63, 0.0026878526), (64, 0.002517429), (65, 0.0031031028), (66, 0.002352533), (67, 0.0019938813)], 17: [(18, 0.03145386), (19, 0.009674273), (20, 0.007164672), (21, 0.004939877), (22, 0.0060025356), (23, 0.0063849185), (24, 0.0035766724), (25, 0.0034180025), (26, 0.0025857016), (27, 0.0032652405), (28, 0.002219168), (29, 0.0025258197), (30, 0.002352523), (31, 0.0021170224), (32, 0.0029947192), (33, 0.0026332173), (34, 0.00223515), (35, 0.0024340446), (36, 0.0020733813), (37, 0.0017848269), (38, 0.0018056832), (39, 0.001983539), (40, 0.002060737), (41, 0.0016778297), (42, 0.0017278874), (43, 0.002104058), (44, 0.0018312446), (45, 0.0017672115), (46, 0.001459241), (47, 0.001527229), (48, 0.0016379235), (49, 0.0013990252), (50, 0.0009848856), (51, 0.0015518632), (52, 0.0012640536), (53, 0.0012749864), (54, 0.0016100928), (55, 0.0011613494), (56, 0.0017035468), (57, 0.001813114), (58, 0.002023044), (59, 0.0018867013), (60, 0.0018497049), (61, 0.002088324), (62, 0.002058566), (63, 0.0024735776), (64, 0.002561088), (65, 0.0027412786), (66, 0.0025062922), (67, 0.002143672)], 18: [(19, 0.021618739), (20, 0.011354166), (21, 0.00543492), (22, 0.0061982316), (23, 0.005582263), (24, 0.0037618205), (25, 0.0038548752), (26, 0.0024653366), (27, 0.00347066), (28, 0.0019734886), (29, 0.002560973), (30, 0.0025443777), (31, 0.002081814), (32, 0.0034531737), (33, 0.0028442093), (34, 0.0030023807), (35, 0.002505333), (36, 0.0026831673), (37, 0.0019678075), (38, 0.0021851524), (39, 0.002297877), (40, 0.002141414), (41, 0.0023825031), (42, 0.002027488), (43, 0.0022182257), (44, 0.001971761), (45, 0.0020282976), (46, 0.0014248159), (47, 0.0017300155), (48, 0.0018241787), (49, 0.0016611667), (50, 0.0010507014), (51, 0.0017270524), (52, 0.0012133004), (53, 0.0011068657), (54, 0.0015652623), (55, 0.00097075815), (56, 0.001520697), (57, 0.0019407296), (58, 0.0016961904), (59, 0.0019088754), (60, 0.0017716461), (61, 0.002143142), (62, 0.0015991253), (63, 0.0023474982), (64, 0.0022198667), (65, 0.0021123372), (66, 0.0024785597), (67, 0.002222665)], 19: [(20, 0.029689206), (21, 0.009761842), (22, 0.0062827934), (23, 0.0063463226), (24, 0.0047753574), (25, 0.004134577), (26, 0.0028504634), (27, 0.0031198862), (28, 0.0021084123), (29, 0.002285095), (30, 0.0022220064), (31, 0.0024528864), (32, 0.0030400008), (33, 0.0032792706), (34, 0.0029648219), (35, 0.0036964607), (36, 0.0025608814), (37, 0.0024943624), (38, 0.0020835993), (39, 0.0023278694), (40, 0.0029063341), (41, 0.0021582867), (42, 0.0022727791), (43, 0.002763964), (44, 0.0022261015), (45, 0.0022685777), (46, 0.0025397867), (47, 0.0019113254), (48, 0.0018045134), (49, 0.0015788252), (50, 0.0011319035), (51, 0.0014358005), (52, 0.0012647181), (53, 0.0012810302), (54, 0.0013172687), (55, 0.0013352999), (56, 0.0013646162), (57, 0.0013584462), (58, 0.0017584597), (59, 0.0014870674), (60, 0.0013678382), (61, 0.0014668675), (62, 0.0014986973), (63, 0.0015838249), (64, 0.0016748145), (65, 0.0019317224), (66, 0.00197688), (67, 0.0018369621)], 20: [(21, 0.021780424), (22, 0.011161942), (23, 0.0076869265), (24, 0.004201983), (25, 0.004763127), (26, 0.0026346697), (27, 0.0043976684), (28, 0.0020206233), (29, 0.0025234907), (30, 0.0024701476), (31, 0.002069585), (32, 0.003815275), (33, 0.0028582127), (34, 0.003281328), (35, 0.0033817042), (36, 0.003524245), (37, 0.00254974), (38, 0.0027620536), (39, 0.0029819428), (40, 0.00287596), (41, 0.002895401), (42, 0.0027756009), (43, 0.0031055296), (44, 0.0032481377), (45, 0.0027635193), (46, 0.0019923323), (47, 0.0023899132), (48, 0.002524141), (49, 0.0021390652), (50, 0.0013244515), (51, 0.0019499124), (52, 0.0013510924), (53, 0.0010852648), (54, 0.0013547335), (55, 0.00081682164), (56, 0.0012798313), (57, 0.0015353818), (58, 0.0012882251), (59, 0.0015618214), (60, 0.0013957989), (61, 0.0015382733), (62, 0.0011780744), (63, 0.0016277405), (64, 0.0015334532), (65, 0.0015803329), (66, 0.0018022475), (67, 0.0017558)], 21: [(22, 0.030375848), (23, 0.0111793475), (24, 0.0055366717), (25, 0.004848764), (26, 0.003919578), (27, 0.0034117606), (28, 0.0022597287), (29, 0.0023129086), (30, 0.0022665386), (31, 0.0020887735), (32, 0.0029493019), (33, 0.0031771895), (34, 0.0027971752), (35, 0.004418684), (36, 0.0035287172), (37, 0.0027514272), (38, 0.0026044394), (39, 0.0029141584), (40, 0.003874065), (41, 0.0026057258), (42, 0.0027204033), (43, 0.0030824838), (44, 0.0030943283), (45, 0.0039917645), (46, 0.0036068705), (47, 0.002932339), (48, 0.0025199975), (49, 0.0022124408), (50, 0.0014298366), (51, 0.0019112285), (52, 0.0015535962), (53, 0.0013634188), (54, 0.0014338279), (55, 0.0010909954), (56, 0.0013763454), (57, 0.0012966405), (58, 0.0015213437), (59, 0.0013054913), (60, 0.0012726192), (61, 0.0013051555), (62, 0.001146287), (63, 0.0014106923), (64, 0.0014862226), (65, 0.0014842802), (66, 0.0016043661), (67, 0.0014885662)], 22: [(23, 0.031057954), (24, 0.006929276), (25, 0.0066151195), (26, 0.0037532926), (27, 0.0050260723), (28, 0.0024560555), (29, 0.0028317247), (30, 0.002672268), (31, 0.0020619147), (32, 0.0032746075), (33, 0.002968689), (34, 0.0031834452), (35, 0.003307718), (36, 0.003807489), (37, 0.0029866695), (38, 0.0033161633), (39, 0.003771486), (40, 0.0032930025), (41, 0.0031450149), (42, 0.0029611003), (43, 0.0032078917), (44, 0.0032881731), (45, 0.0032019), (46, 0.0028648716), (47, 0.003565642), (48, 0.003229486), (49, 0.0028946616), (50, 0.0017496472), (51, 0.0024676372), (52, 0.0016948633), (53, 0.0013502464), (54, 0.0016760225), (55, 0.0009800184), (56, 0.0014362647), (57, 0.0015405793), (58, 0.0012353095), (59, 0.001369635), (60, 0.0013126318), (61, 0.0016093251), (62, 0.0010428899), (63, 0.0014304658), (64, 0.0014545893), (65, 0.001361965), (66, 0.00154499), (67, 0.0013868574)], 23: [(24, 0.020230379), (25, 0.008849557), (26, 0.0047787875), (27, 0.004996523), (28, 0.002897392), (29, 0.0028866197), (30, 0.0027399512), (31, 0.0021210578), (32, 0.0029676661), (33, 0.003033333), (34, 0.0030802812), (35, 0.00396708), (36, 0.0035269258), (37, 0.003044604), (38, 0.0033172101), (39, 0.0037647514), (40, 0.0040875804), (41, 0.0029259913), (42, 0.0028251729), (43, 0.0032299298), (44, 0.003217613), (45, 0.0030987514), (46, 0.0030801692), (47, 0.0031265628), (48, 0.0032315599), (49, 0.0029905613), (50, 0.0017418413), (51, 0.0024492627), (52, 0.0021326388), (53, 0.0015298651), (54, 0.001713918), (55, 0.0010878921), (56, 0.0013224232), (57, 0.0013737084), (58, 0.0015036371), (59, 0.0011442617), (60, 0.0011671722), (61, 0.0012910679), (62, 0.0010932955), (63, 0.0011533375), (64, 0.0012778044), (65, 0.0012973209), (66, 0.0014122088), (67, 0.0011973989)], 24: [(25, 0.02846746), (26, 0.007957367), (27, 0.006597851), (28, 0.0037473992), (29, 0.0035501488), (30, 0.0028898579), (31, 0.0025288626), (32, 0.0032333962), (33, 0.003288411), (34, 0.0034219027), (35, 0.0038049463), (36, 0.0036132857), (37, 0.0030957418), (38, 0.003348188), (39, 0.0038560403), (40, 0.0037625625), (41, 0.0029181943), (42, 0.0028342009), (43, 0.002975795), (44, 0.002744602), (45, 0.0032365005), (46, 0.0032451625), (47, 0.0029037416), (48, 0.0030178337), (49, 0.0031044942), (50, 0.0023707543), (51, 0.002762558), (52, 0.0020975024), (53, 0.0021381644), (54, 0.0018083298), (55, 0.0013732582), (56, 0.0014355369), (57, 0.0014965396), (58, 0.0016600997), (59, 0.0013331725), (60, 0.0012780732), (61, 0.0013676345), (62, 0.0011328713), (63, 0.0014017137), (64, 0.0014269935), (65, 0.0014035262), (66, 0.0014400622), (67, 0.0013140806)], 25: [(26, 0.019262694), (27, 0.009727028), (28, 0.003981362), (29, 0.0039972537), (30, 0.0036541822), (31, 0.0025989297), (32, 0.0041566924), (33, 0.0031771336), (34, 0.0036617846), (35, 0.0035616334), (36, 0.004094081), (37, 0.0033008961), (38, 0.0034962383), (39, 0.0040223203), (40, 0.0034790288), (41, 0.0032214725), (42, 0.0028391748), (43, 0.0027778738), (44, 0.0029208346), (45, 0.002598528), (46, 0.0021410438), (47, 0.00290716), (48, 0.0028418873), (49, 0.0029787968), (50, 0.0019029797), (51, 0.003208029), (52, 0.0021613077), (53, 0.0017905869), (54, 0.0021144014), (55, 0.0011741576), (56, 0.001775099), (57, 0.0017796826), (58, 0.0015103134), (59, 0.0015164488), (60, 0.0015613687), (61, 0.0015749572), (62, 0.0012888612), (63, 0.0015253657), (64, 0.0015293128), (65, 0.0014488976), (66, 0.0015103109), (67, 0.0013867671)], 26: [(27, 0.02829255), (28, 0.006941567), (29, 0.004665472), (30, 0.003647885), (31, 0.0034971419), (32, 0.0034510696), (33, 0.004345186), (34, 0.0030735414), (35, 0.004430816), (36, 0.003622812), (37, 0.0033852097), (38, 0.0034811962), (39, 0.0033848637), (40, 0.003961716), (41, 0.0024030977), (42, 0.002324121), (43, 0.002482619), (44, 0.0022314608), (45, 0.0029070654), (46, 0.0033457798), (47, 0.0023566473), (48, 0.0024391413), (49, 0.002238345), (50, 0.0018519658), (51, 0.0030278175), (52, 0.0027379643), (53, 0.0024668325), (54, 0.0021347508), (55, 0.0018044212), (56, 0.001860038), (57, 0.0017672802), (58, 0.002058731), (59, 0.0015394691), (60, 0.0015052652), (61, 0.0016484739), (62, 0.0012303862), (63, 0.0015565049), (64, 0.0016051627), (65, 0.0015082632), (66, 0.0014426358), (67, 0.001207131)], 27: [(28, 0.018192407), (29, 0.0076352693), (30, 0.005221838), (31, 0.0041362667), (32, 0.006184758), (33, 0.0038024285), (34, 0.0040909774), (35, 0.003865951), (36, 0.00404684), (37, 0.0033454946), (38, 0.0035178948), (39, 0.004056487), (40, 0.0035690046), (41, 0.0028624218), (42, 0.0026278505), (43, 0.0026820865), (44, 0.0025832993), (45, 0.0021782115), (46, 0.0015762446), (47, 0.002261895), (48, 0.0022969057), (49, 0.0025398058), (50, 0.0015503926), (51, 0.002799117), (52, 0.0020338888), (53, 0.0018844381), (54, 0.0023788002), (55, 0.0013035146), (56, 0.002220641), (57, 0.002237303), (58, 0.0017924001), (59, 0.0018031999), (60, 0.0020063734), (61, 0.0019086367), (62, 0.0013457205), (63, 0.0017546691), (64, 0.0018601149), (65, 0.001672421), (66, 0.0016246978), (67, 0.0015559557)], 28: [(29, 0.02644872), (30, 0.0069810473), (31, 0.0071873264), (32, 0.0072206096), (33, 0.0057990686), (34, 0.0045280834), (35, 0.0046246373), (36, 0.0034536168), (37, 0.003055692), (38, 0.003073886), (39, 0.0034088676), (40, 0.0038927346), (41, 0.0025434392), (42, 0.0023856235), (43, 0.0026452416), (44, 0.002110825), (45, 0.0025267813), (46, 0.0022341001), (47, 0.00188122), (48, 0.0019489299), (49, 0.0019113797), (50, 0.0014195662), (51, 0.0021006253), (52, 0.0020819549), (53, 0.002321944), (54, 0.0021700219), (55, 0.0019051272), (56, 0.0021771768), (57, 0.0024595368), (58, 0.0023257434), (59, 0.002004326), (60, 0.0020114381), (61, 0.0019946976), (62, 0.0016074013), (63, 0.0018741417), (64, 0.0019956923), (65, 0.0022646687), (66, 0.0020203388), (67, 0.0017466458)], 29: [(30, 0.016158553), (31, 0.0086131785), (32, 0.009839837), (33, 0.006531039), (34, 0.0069903317), (35, 0.004934388), (36, 0.004873811), (37, 0.0033027902), (38, 0.0034091189), (39, 0.003998647), (40, 0.003365067), (41, 0.003303013), (42, 0.0027062728), (43, 0.0025703537), (44, 0.002403788), (45, 0.0020117548), (46, 0.0017268895), (47, 0.0021467821), (48, 0.0019956355), (49, 0.0022566915), (50, 0.0012814136), (51, 0.0019475664), (52, 0.0015840564), (53, 0.0016126506), (54, 0.0022844095), (55, 0.0012408688), (56, 0.002275672), (57, 0.0026381528), (58, 0.002074202), (59, 0.0021693697), (60, 0.0024836485), (61, 0.0025347273), (62, 0.0016181755), (63, 0.0024018523), (64, 0.0022903597), (65, 0.0033206781), (66, 0.002283849), (67, 0.0020078467)], 30: [(31, 0.021916954), (32, 0.012616654), (33, 0.006693253), (34, 0.007011896), (35, 0.005298955), (36, 0.0060650446), (37, 0.003705488), (38, 0.0032201584), (39, 0.0038305533), (40, 0.0032641639), (41, 0.003089805), (42, 0.0027806098), (43, 0.0026046515), (44, 0.002528995), (45, 0.002146453), (46, 0.0017388257), (47, 0.0019831397), (48, 0.0020920604), (49, 0.0019545003), (50, 0.0013391054), (51, 0.002032917), (52, 0.0015297977), (53, 0.0015487263), (54, 0.0021153719), (55, 0.0013291839), (56, 0.0021537126), (57, 0.0023873504), (58, 0.0021203302), (59, 0.0023936515), (60, 0.0023095785), (61, 0.0026697435), (62, 0.0019029819), (63, 0.0023961011), (64, 0.0024762324), (65, 0.002377949), (66, 0.0026209995), (67, 0.0021225687)], 31: [(32, 0.028454186), (33, 0.008594628), (34, 0.0055156904), (35, 0.0071713887), (36, 0.005345183), (37, 0.004587911), (38, 0.0032220003), (39, 0.003354232), (40, 0.004334399), (41, 0.0024873954), (42, 0.002301174), (43, 0.0028603708), (44, 0.002136389), (45, 0.0025635445), (46, 0.0024007282), (47, 0.0018293564), (48, 0.0017704709), (49, 0.0014780624), (50, 0.0011690853), (51, 0.0015346594), (52, 0.0014785369), (53, 0.0016093368), (54, 0.0016411288), (55, 0.0017732391), (56, 0.0017242829), (57, 0.0018109797), (58, 0.0021551088), (59, 0.0018051687), (60, 0.001899634), (61, 0.0018113089), (62, 0.0019744674), (63, 0.002079933), (64, 0.0024099478), (65, 0.002974899), (66, 0.002469565), (67, 0.0021994933)], 32: [(33, 0.019862268), (34, 0.008900548), (35, 0.006518255), (36, 0.006762298), (37, 0.0047456105), (38, 0.004378411), (39, 0.0051596635), (40, 0.0041397884), (41, 0.0037846656), (42, 0.0031817963), (43, 0.003070962), (44, 0.002702527), (45, 0.0023903528), (46, 0.0017165019), (47, 0.0020331778), (48, 0.0020869395), (49, 0.0019642166), (50, 0.0012386428), (51, 0.0021002719), (52, 0.001414634), (53, 0.0012755282), (54, 0.0016729375), (55, 0.0010670783), (56, 0.0016726052), (57, 0.0017857667), (58, 0.0016138846), (59, 0.0019243526), (60, 0.0018471344), (61, 0.0020584858), (62, 0.0014024602), (63, 0.0020785246), (64, 0.0022240242), (65, 0.0022331811), (66, 0.0024473416), (67, 0.0021492387)], 33: [(34, 0.025667049), (35, 0.010045992), (36, 0.005868306), (37, 0.0045406017), (38, 0.0041968897), (39, 0.004945352), (40, 0.005270195), (41, 0.0035519197), (42, 0.0037219657), (43, 0.003156757), (44, 0.0027250822), (45, 0.0035812522), (46, 0.0034274035), (47, 0.0022776164), (48, 0.002013852), (49, 0.0017817765), (50, 0.0013016405), (51, 0.0017584044), (52, 0.001514538), (53, 0.0015992677), (54, 0.0015522701), (55, 0.0014549377), (56, 0.001469023), (57, 0.0014723282), (58, 0.0020860212), (59, 0.0015251096), (60, 0.0014778354), (61, 0.0015391636), (62, 0.0014095992), (63, 0.0016916912), (64, 0.0018250326), (65, 0.0019909583), (66, 0.0022104848), (67, 0.0018128937)], 34: [(35, 0.020337146), (36, 0.009044204), (37, 0.005452424), (38, 0.004820623), (39, 0.0062884777), (40, 0.004771059), (41, 0.0048584975), (42, 0.0046975757), (43, 0.0040101586), (44, 0.0035819262), (45, 0.0029610067), (46, 0.0021964523), (47, 0.002588273), (48, 0.0026368794), (49, 0.0024750715), (50, 0.0013372629), (51, 0.0023498188), (52, 0.0014751976), (53, 0.0012583517), (54, 0.00163122), (55, 0.0009437953), (56, 0.0017096602), (57, 0.0016529687), (58, 0.0014987906), (59, 0.0016406728), (60, 0.0015978545), (61, 0.0019727198), (62, 0.001265726), (63, 0.0021098126), (64, 0.0020578532), (65, 0.0016685779), (66, 0.0021926325), (67, 0.0018180646)], 35: [(36, 0.030839244), (37, 0.008597763), (38, 0.0050559924), (39, 0.005343178), (40, 0.0060198843), (41, 0.0040773195), (42, 0.0037068992), (43, 0.0039338893), (44, 0.0041412506), (45, 0.004125887), (46, 0.0037884945), (47, 0.0027143022), (48, 0.0024683464), (49, 0.002292313), (50, 0.0014168066), (51, 0.0017723978), (52, 0.0016199419), (53, 0.001446105), (54, 0.0014180577), (55, 0.0011811608), (56, 0.0013980306), (57, 0.0012925045), (58, 0.0015728751), (59, 0.0014202674), (60, 0.001338626), (61, 0.00137152), (62, 0.0011843309), (63, 0.0016034826), (64, 0.0016207419), (65, 0.0016368701), (66, 0.0017321935), (67, 0.0015133242)], 36: [(37, 0.024566427), (38, 0.007990102), (39, 0.007650439), (40, 0.0054934034), (41, 0.004892757), (42, 0.0044861967), (43, 0.00403087), (44, 0.004229195), (45, 0.0037171303), (46, 0.002972038), (47, 0.0035993394), (48, 0.0029920123), (49, 0.003103074), (50, 0.0016167047), (51, 0.0024559333), (52, 0.0017128831), (53, 0.0013861235), (54, 0.0017071086), (55, 0.0008895703), (56, 0.0016546039), (57, 0.0013158795), (58, 0.0012420835), (59, 0.0013603774), (60, 0.001436406), (61, 0.0014173636), (62, 0.001093632), (63, 0.0015643452), (64, 0.0017320357), (65, 0.0014123884), (66, 0.0017673358), (67, 0.0014716197)], 37: [(38, 0.019398268), (39, 0.009417624), (40, 0.0070108296), (41, 0.0044411905), (42, 0.0036319115), (43, 0.004315103), (44, 0.0035635715), (45, 0.0039255894), (46, 0.004856134), (47, 0.0039605065), (48, 0.0036991192), (49, 0.00284954), (50, 0.0019035448), (51, 0.0023273581), (52, 0.0019712427), (53, 0.001664282), (54, 0.001545135), (55, 0.0014142111), (56, 0.0013566663), (57, 0.0013378753), (58, 0.0013408771), (59, 0.0012882234), (60, 0.0012630502), (61, 0.0013131489), (62, 0.0011247046), (63, 0.0014306805), (64, 0.0014029477), (65, 0.0015667918), (66, 0.001364269), (67, 0.0013697766)], 38: [(39, 0.026923234), (40, 0.0092510125), (41, 0.0058136135), (42, 0.0050080796), (43, 0.0048768055), (44, 0.0047035585), (45, 0.0045355167), (46, 0.004739058), (47, 0.0052020424), (48, 0.005034068), (49, 0.0037579418), (50, 0.0024211132), (51, 0.0029102368), (52, 0.0020443117), (53, 0.0018156056), (54, 0.001772672), (55, 0.0011088765), (56, 0.0014458993), (57, 0.0012746106), (58, 0.0015389969), (59, 0.0012394761), (60, 0.001157029), (61, 0.0011806259), (62, 0.0010920952), (63, 0.001258813), (64, 0.0013831276), (65, 0.0012520552), (66, 0.0014438523), (67, 0.0012341167)], 39: [(40, 0.02055054), (41, 0.008059664), (42, 0.0065836986), (43, 0.005267733), (44, 0.005427442), (45, 0.0047857277), (46, 0.0040687406), (47, 0.0054865438), (48, 0.0054849605), (49, 0.0045587746), (50, 0.0022896973), (51, 0.0034102977), (52, 0.0020617424), (53, 0.0015565874), (54, 0.0017430148), (55, 0.0009922985), (56, 0.0014750137), (57, 0.001417798), (58, 0.0012265525), (59, 0.0015438269), (60, 0.0012871334), (61, 0.0014053412), (62, 0.0009930583), (63, 0.0015839925), (64, 0.001384972), (65, 0.0015966815), (66, 0.001534877), (67, 0.0012511159)], 40: [(41, 0.025047725), (42, 0.009750377), (43, 0.0063092588), (44, 0.005682022), (45, 0.006776318), (46, 0.0066607846), (47, 0.0056002713), (48, 0.0060552484), (49, 0.0045731636), (50, 0.002392211), (51, 0.0027830144), (52, 0.0022665677), (53, 0.0020179476), (54, 0.0018531919), (55, 0.0013874124), (56, 0.001607186), (57, 0.0014389361), (58, 0.0015628429), (59, 0.0013362962), (60, 0.001374081), (61, 0.0012758743), (62, 0.0010325203), (63, 0.0013547158), (64, 0.0014228318), (65, 0.001379936), (66, 0.0014876517), (67, 0.0013507239)], 41: [(42, 0.029090533), (43, 0.0089335935), (44, 0.007105041), (45, 0.006041987), (46, 0.004608366), (47, 0.005652093), (48, 0.005845904), (49, 0.0058347997), (50, 0.0024825104), (51, 0.0035920504), (52, 0.0020993922), (53, 0.0017462226), (54, 0.0022235664), (55, 0.0010470691), (56, 0.0017209183), (57, 0.0018545947), (58, 0.0015034835), (59, 0.0013632824), (60, 0.0014852696), (61, 0.0017920849), (62, 0.0011535219), (63, 0.0016511456), (64, 0.0016429193), (65, 0.0014132883), (66, 0.001767329), (67, 0.0017078433)], 42: [(43, 0.02244976), (44, 0.009923953), (45, 0.0068429676), (46, 0.004915457), (47, 0.0056750015), (48, 0.0058180974), (49, 0.0059705316), (50, 0.002735368), (51, 0.0035458219), (52, 0.0020435634), (53, 0.0016402819), (54, 0.0019399867), (55, 0.0010689157), (56, 0.0015125129), (57, 0.0017465856), (58, 0.0013935637), (59, 0.0014659726), (60, 0.0013563309), (61, 0.0015789894), (62, 0.0010679385), (63, 0.0015295203), (64, 0.0014366457), (65, 0.001895053), (66, 0.0017357906), (67, 0.0016455066)], 43: [(44, 0.029793395), (45, 0.01152275), (46, 0.0069241486), (47, 0.00639949), (48, 0.005566009), (49, 0.0053576906), (50, 0.003238581), (51, 0.0034115415), (52, 0.0023922992), (53, 0.0018886626), (54, 0.001868503), (55, 0.0012310096), (56, 0.0016803042), (57, 0.00142697), (58, 0.0015439643), (59, 0.0014277404), (60, 0.00137892), (61, 0.0012633934), (62, 0.0011460134), (63, 0.0013729227), (64, 0.0014922408), (65, 0.0013236544), (66, 0.0016316865), (67, 0.0015105712)], 44: [(45, 0.029994214), (46, 0.008381319), (47, 0.00791751), (48, 0.006597843), (49, 0.0058386135), (50, 0.00309207), (51, 0.0039395955), (52, 0.0024539975), (53, 0.0017860573), (54, 0.0019422583), (55, 0.0011044199), (56, 0.0016529706), (57, 0.0015942011), (58, 0.0012902041), (59, 0.0015988679), (60, 0.0013850895), (61, 0.0014038766), (62, 0.0010086535), (63, 0.0015089655), (64, 0.0014069745), (65, 0.0013242153), (66, 0.001611847), (67, 0.00149502)], 45: [(46, 0.024442878), (47, 0.0093558375), (48, 0.0072890874), (49, 0.006051158), (50, 0.0033034293), (51, 0.0037753514), (52, 0.0025682612), (53, 0.002080752), (54, 0.002125894), (55, 0.0012972055), (56, 0.001642895), (57, 0.0015435205), (58, 0.0014067357), (59, 0.0013097962), (60, 0.0013199324), (61, 0.0013507139), (62, 0.0010011868), (63, 0.0013606315), (64, 0.0014893789), (65, 0.0013148102), (66, 0.0014375875), (67, 0.0013956844)], 46: [(47, 0.034665354), (48, 0.011488098), (49, 0.0065063178), (50, 0.0038899959), (51, 0.0037360385), (52, 0.0026193387), (53, 0.0020640674), (54, 0.0016857801), (55, 0.001484034), (56, 0.0011696861), (57, 0.0012737004), (58, 0.0021867866), (59, 0.0010892983), (60, 0.0009769346), (61, 0.00097320887), (62, 0.00093745923), (63, 0.0009597696), (64, 0.0010103778), (65, 0.0012347508), (66, 0.0011871668), (67, 0.0011992569)], 47: [(48, 0.033955175), (49, 0.010585122), (50, 0.004325716), (51, 0.0055944663), (52, 0.0032346086), (53, 0.0022660464), (54, 0.002127129), (55, 0.0012981481), (56, 0.0014733387), (57, 0.0013668396), (58, 0.0013732653), (59, 0.0015661102), (60, 0.0011556435), (61, 0.0012417057), (62, 0.000977074), (63, 0.0012863635), (64, 0.00116711), (65, 0.0010922767), (66, 0.0013208083), (67, 0.0011004023)], 48: [(49, 0.03260075), (50, 0.0059881336), (51, 0.0063997707), (52, 0.0052558216), (53, 0.003522252), (54, 0.0025139465), (55, 0.0015535073), (56, 0.0016601294), (57, 0.0014973347), (58, 0.0014260566), (59, 0.0012771388), (60, 0.001389644), (61, 0.0012339527), (62, 0.0009945094), (63, 0.001192633), (64, 0.0011995939), (65, 0.0012140714), (66, 0.0010830442), (67, 0.0011154214)], 49: [(50, 0.01562233), (51, 0.008826251), (52, 0.0043990565), (53, 0.0030132174), (54, 0.0029625138), (55, 0.0013416102), (56, 0.0019870948), (57, 0.0020477385), (58, 0.0014918075), (59, 0.0015211793), (60, 0.0012552296), (61, 0.0016153549), (62, 0.0010086191), (63, 0.0013879644), (64, 0.0013653719), (65, 0.0011356865), (66, 0.0013836548), (67, 0.0012075945)], 50: [(51, 0.024674954), (52, 0.0063978345), (53, 0.0048855827), (54, 0.0036006279), (55, 0.002152002), (56, 0.0019851858), (57, 0.0016237436), (58, 0.0021072093), (59, 0.0015291051), (60, 0.0013302286), (61, 0.00118675), (62, 0.0011244502), (63, 0.0013581029), (64, 0.0012948685), (65, 0.0010955571), (66, 0.0012693423), (67, 0.0011570845)], 51: [(52, 0.021014038), (53, 0.0048361868), (54, 0.0046302504), (55, 0.002033002), (56, 0.0030796456), (57, 0.002431649), (58, 0.0016661967), (59, 0.0021413555), (60, 0.0020253402), (61, 0.0017843967), (62, 0.001212947), (63, 0.0017197848), (64, 0.0015794196), (65, 0.001270435), (66, 0.0014285791), (67, 0.0015510219)], 52: [(53, 0.018750722), (54, 0.0061429846), (55, 0.0038061512), (56, 0.004214831), (57, 0.0027535632), (58, 0.0026515513), (59, 0.0020895049), (60, 0.0019395319), (61, 0.001708188), (62, 0.0015224963), (63, 0.0014649829), (64, 0.0015544187), (65, 0.0015201496), (66, 0.0014381632), (67, 0.0015506614)], 53: [(54, 0.02595818), (55, 0.0059480234), (56, 0.0058188005), (57, 0.005313462), (58, 0.005092035), (59, 0.003036142), (60, 0.0024475863), (61, 0.0022526497), (62, 0.0018276272), (63, 0.0020060576), (64, 0.0019138352), (65, 0.0016670143), (66, 0.0016986108), (67, 0.0016117056)], 54: [(55, 0.0115882885), (56, 0.007161409), (57, 0.0066154003), (58, 0.00450269), (59, 0.004478484), (60, 0.0038216752), (61, 0.0037155773), (62, 0.0022644822), (63, 0.002999369), (64, 0.0025315387), (65, 0.0027361396), (66, 0.0023560326), (67, 0.0019567045)], 55: [(56, 0.02217265), (57, 0.009054403), (58, 0.00869692), (59, 0.0063047037), (60, 0.0052787964), (61, 0.0041880817), (62, 0.0041753203), (63, 0.003907963), (64, 0.0030760665), (65, 0.003023923), (66, 0.0022775976), (67, 0.0024269493)], 56: [(57, 0.031859655), (58, 0.008580144), (59, 0.007797538), (60, 0.006262015), (61, 0.005688583), (62, 0.003910574), (63, 0.0046627703), (64, 0.004873751), (65, 0.003509044), (66, 0.0030549017), (67, 0.003142189)], 57: [(58, 0.020666229), (59, 0.009411817), (60, 0.0073327324), (61, 0.0061358283), (62, 0.0041208835), (63, 0.0047756857), (64, 0.0046351296), (65, 0.003689165), (66, 0.0037276503), (67, 0.0037165822)], 58: [(59, 0.02766441), (60, 0.00957056), (61, 0.0074370787), (62, 0.0056137787), (63, 0.005311119), (64, 0.0048494446), (65, 0.0043075224), (66, 0.0042320616), (67, 0.0040502106)], 59: [(60, 0.025015581), (61, 0.010877352), (62, 0.006908168), (63, 0.0064932965), (64, 0.005884486), (65, 0.004176465), (66, 0.0044463743), (67, 0.004332628)], 60: [(61, 0.029041514), (62, 0.008995329), (63, 0.007352674), (64, 0.006663091), (65, 0.0049273865), (66, 0.0047741556), (67, 0.004896535)], 61: [(62, 0.019661736), (63, 0.009366417), (64, 0.009353369), (65, 0.0054909526), (66, 0.0051351152), (67, 0.0044509103)], 62: [(63, 0.023570776), (64, 0.010907724), (65, 0.008297064), (66, 0.0056652804), (67, 0.004995022)], 63: [(64, 0.029186727), (65, 0.009273237), (66, 0.0066153486), (67, 0.004911336)], 64: [(65, 0.025863554), (66, 0.008671388), (67, 0.0052596657)], 65: [(66, 0.025000924), (67, 0.008035478)], 66: [(67, 0.024103446)]})\n",
      "len(tokenids):271\n",
      "len(tokentokenid2word_mapping):271\n",
      "attention_matrix:(271, 271)\n",
      "merged_attention shape:(68, 68)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "30",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-48d1c8b08488>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhead_tail_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mtail_head_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead_tail_pairs_append\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtail_head_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid2noun_chunk_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_chunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-48d1c8b08488>\u001b[0m in \u001b[0;36mhead_tail_pairs_append\u001b[0;34m(head_tail_pairs, sent, id2noun_chunk_token, start_chunk, token2id)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m#                 print (\"succeed, start1 :%s,start2:%s\"%(noun_chunks[i1],noun_chunks[i2]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;31m#                         print ('sc')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                         \u001b[0mhead_tail_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2noun_chunk_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid2noun_chunk_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhead_tail_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 30"
     ]
    }
   ],
   "source": [
    "language_model = 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(language_model)\n",
    "encoder = BertModel.from_pretrained(language_model)\n",
    "encoder.eval()\n",
    "use_cuda =False\n",
    "threshold = 0.01\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# def process_matrix(attentions, layer_idx = -1, head_num = 0, avg_head=False, trim=True, use_cuda=True):\n",
    "#     if avg_head:\n",
    "#         if use_cuda:\n",
    "#             attn =  torch.mean(attentions[0][layer_idx], 0).cpu()\n",
    "#         else:\n",
    "#             attn = torch.mean(attentions[0][layer_idx], 0)\n",
    "#         attention_matrix = attn.detach().numpy()\n",
    "#     else:\n",
    "#         attn = attentions[0][layer_idx][head_num]\n",
    "#         if use_cuda:\n",
    "#             attn = attn.cpu()\n",
    "#         attention_matrix = attn.detach().numpy()\n",
    "\n",
    "#     attention_matrix = attention_matrix[1:-1, 1:-1]\n",
    "\n",
    "#     return attention_matrix\n",
    "\n",
    "\n",
    "###test ###\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher1 = PhraseMatcher(nlp.vocab,attr='POS')\n",
    "\n",
    "terms = ['relate to','positioned through','engageable with','so as to','makes it possible to']\n",
    "patterns=[nlp(term) for term in terms]\n",
    "matcher1.add('VerbPhrase',patterns)\n",
    "\n",
    "\n",
    "\n",
    "###test ###\n",
    "\n",
    "\n",
    "with open ('examples/patent_example.txt','r') as f:\n",
    "    num_file = sum([1 for i in open('examples/patent_example.txt', \"r\")])\n",
    "    #tqdm summmarization\n",
    "    for idx, line in enumerate(tqdm (f,total=num_file)):\n",
    "\n",
    "        ##modification starts below###\n",
    "#         sentence= line.strip()\n",
    "        sentence = re.sub(' +', ' ',line.strip())\n",
    "        \n",
    "        ##modification ends here\\####\n",
    "#         if len(sentence):\n",
    "#             valid_triplets = []\n",
    "#             for sent in nlp(sentence).sents:\n",
    "#                 sentence= sent.text\n",
    "#                 tokenizer_name = str(tokenizer.__str__)\n",
    "#                 inputs, tokenid2word_mapping, token2id, noun_chunks  = create_mapping(sentence, return_pt=True, nlp=nlp, tokenizer=tokenizer)\n",
    "\n",
    "#                 with torch.no_grad():\n",
    "#                     if use_cuda:\n",
    "#                         for key in inputs.keys():\n",
    "#                             inputs[key] = inputs[key].cuda()\n",
    "#                     outputs = encoder(**inputs, output_attentions=True)\n",
    "#                 trim = True\n",
    "#                 if 'GPT2' in tokenizer_name:\n",
    "#                     trim  = False\n",
    "\n",
    "#                 '''\n",
    "#                 Use average of last layer attention : page 6, section 3.1.2\n",
    "#                 '''\n",
    "#                 attention = process_matrix(outputs[2], avg_head=True, trim=trim, use_cuda=use_cuda)\n",
    "\n",
    "#                 merged_attention = compress_attention(attention, tokenid2word_mapping)\n",
    "#                 attn_graph = build_graph(merged_attention)\n",
    "#             print ('attention:{}'.format(attention))\n",
    "#             print ('merged_attention:{}'.format(merged_attention))\n",
    "#             print (\"attn_graph:{}\".format(attn_graph))\n",
    "#             print (\"attention_shape:{0},merged_attention_shape:{1},attention_graph_shape:{2}\".\\\n",
    "#                    format(attention.shape,              \n",
    "#                           merged_attention.shape,\n",
    "#                          len(attn_graph)))\n",
    "\n",
    "        valid_triplets = []\n",
    "        count = 0\n",
    "        doc = nlp(sentence)\n",
    "        print (\"count:{}\".format(count))\n",
    "        \n",
    "        ###test for verb phrases start###\n",
    "        matches = matcher1(doc)\n",
    "        verb_phrase_start_chunk = []\n",
    "        verb_phrase_end_chunk = []\n",
    "        verb_phrase_chunk=[]\n",
    "        for match_id, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            verb_phrase_start_chunk.append(start)\n",
    "            verb_phrase_end_chunk.append(end)\n",
    "            verb_phrase_chunk.append(span.text)\n",
    "            # print(\"start: %s, end: %s text :%s\" % (start,end,span.text))\n",
    "            \n",
    "        ### test for verb phrase ends####\n",
    "        \n",
    "        \n",
    "        \n",
    "        for sent in nlp(sentence).sents:\n",
    "            print (\"sent:{}\".format(sent))\n",
    "            print (\"sent start :%s sent end :%s\"% (sent.start, sent.end))\n",
    "            \n",
    "#         start_chunk = []\n",
    "#         end_chunk = []\n",
    "#         noun_chunks = []\n",
    "\n",
    "        noun_start_chunk = []\n",
    "        noun_end_chunk = []\n",
    "        noun_chunks = []\n",
    "\n",
    "        for chunk in doc.noun_chunks:\n",
    "            noun_chunks.append(chunk.text)\n",
    "            start_chunk.append(chunk.start)\n",
    "            end_chunk.append(chunk.end)\n",
    "            \n",
    "#         start_chunk= sorted(start_chunk + verb_phrase_start_chunk)\n",
    "#         end_chunk = sorted (end_chunk +verb_phrase_end_chunk)\n",
    "        chunk_id = 0\n",
    "        token2id ={ }\n",
    "        mode = 1\n",
    "        sentence_mapping = []\n",
    "\n",
    "        #### modification starts below,\n",
    "        ### token2id [sentence_mapping[-1]]= len(token2id) --->token2id [sentence_mapping[-1]]= len(sentence_mapping)-1\n",
    "        ### because token2id is a dictionary, when you len it, it will automatically remove the duplicated keys, this will\n",
    "        ### cause problems when the words occurs more than once\n",
    "\n",
    "            \n",
    "        dic = dict (zip (start_chunk+verb_phrase_start_chunk,noun_chunks+verb_phrase_chunk))\n",
    "        print (dic)\n",
    "        dic = {k: v for k, v in sorted(dic.items(), key=lambda dic:dic[0],reverse= False)}\n",
    "\n",
    " \n",
    "        start_chunk= list(dic.keys())\n",
    "        chunks = list(dic.values())\n",
    "        end_chunk = sorted(end_chunk+ verb_phrase_end_chunk)\n",
    "        print ('start_chunk: %s, end_chunk: %s'%(str(start_chunk),str(end_chunk)))\n",
    "        print (dic)\n",
    "        for idx,token in enumerate (doc):\n",
    "            if idx in start_chunk:\n",
    "                \n",
    "\n",
    "                mode= 1\n",
    "                sentence_mapping.append (chunks[chunk_id])\n",
    "                token2id [sentence_mapping[-1]]= len(sentence_mapping)-1\n",
    "#                 token2id [sentence_mapping[-1]]= len(token2id)\n",
    "                chunk_id +=1 \n",
    "                continue\n",
    "            if idx in end_chunk:\n",
    "                mode = 0\n",
    "\n",
    "                \n",
    "            if mode == 0:\n",
    "                sentence_mapping.append (token.text)\n",
    "                token2id [sentence_mapping[-1]]= len(sentence_mapping)-1\n",
    "#                 token2id [sentence_mapping[-1]]= len(token2id)\n",
    "\n",
    "        ### modication ends here \n",
    "                \n",
    "        print ('sentence_mapping:{}'.format (sentence_mapping))\n",
    "        \n",
    "        token_ids = []\n",
    "        tokenid2word_mapping = []\n",
    "        count = 0\n",
    "        for token in sentence_mapping:\n",
    "            subtoken_ids = tokenizer(str(token), add_special_tokens=True)['input_ids']\n",
    "            tokenid2word_mapping += [ token2id[token] ]*len(subtoken_ids)  # have a look at the following sample result and you will know.\n",
    "            token_ids += subtoken_ids\n",
    "#             print (\"token_ids:{}\".format(token_ids))\n",
    "#             print (\"tokenid2word_mapping:{}\".format(tokenid2word_mapping))\n",
    "#             print (\"subtoken_ids:{}\".format (subtoken_ids))\n",
    "#             print (\"----next round----\")\n",
    "#             count +=1\n",
    "#             if count ==3 :\n",
    "#                 break\n",
    "\n",
    "                \n",
    "        print (\"noun_chunks:{}\".format(noun_chunks))\n",
    "        print (\"start_chunk:{}\".format(start_chunk))\n",
    "        print (\"end_chunk:{}\".format(end_chunk))\n",
    "        print (\"token2id:{}\".format(token2id))\n",
    "        print (\"token_ids:{}\".format(token_ids))\n",
    "        print (\"tokenid2word_mapping:{}\".format(tokenid2word_mapping))\n",
    "        print (\"subtoken_ids:{}\".format (subtoken_ids))\n",
    "        tokenizer_name = str(tokenizer.__str__)\n",
    "\n",
    "        outputs = {\n",
    "            'input_ids': [tokenizer.cls_token_id] + token_ids + [tokenizer.sep_token_id],\n",
    "            'attention_mask': [1]*(len(token_ids)+2),\n",
    "            'token_type_ids': [0]*(len(token_ids)+2)     }\n",
    "\n",
    "        for key, value in outputs.items():\n",
    "            outputs[key] = torch.from_numpy(np.array(value)).long().unsqueeze(0)\n",
    "        print(\"outputs:{}\".format(outputs))\n",
    "        print (\"len(tokenids):{}\".format(len(token_ids))) #equals to the length of original attn matrix len# \n",
    "        use_cuda = False\n",
    "        with torch.no_grad():\n",
    "            if use_cuda:\n",
    "                for key in outputs.keys():\n",
    "                    outputs[key] = outputs[key].cuda()\n",
    "            outputs_encoder = encoder(**outputs, output_attentions=True)\n",
    "#         print (\"outputs_encoder:{}\".format(outputs_encoder))\n",
    "#         print (\"outputs_encoder[2]:{}\".format(outputs_encoder[2]))        \n",
    "        trim =True\n",
    "        attn = outputs_encoder[2][0][-1]\n",
    "        print (\"attn:{}\".format(attn))\n",
    "        attn =torch.mean(attn, 0)\n",
    "        attention_matrix = attn.detach().numpy()\n",
    "        attention_matrix = attention_matrix[1:-1, 1:-1]\n",
    "\n",
    "#         print (\"attn:{}\".format(attn))    \n",
    "#         print ('attention_matrix:{}'.format(attention_matrix))\n",
    "        print (attention_matrix.shape)\n",
    "        merged_attention = compress_attention(attention_matrix, tokenid2word_mapping)\n",
    "        print (\"merged_attention:{},shape:{}\".format (merged_attention,merged_attention.shape))\n",
    "        attn_graph = build_graph(merged_attention)\n",
    "        print (\"attn_graph:{}\".format(attn_graph))\n",
    "        tail_head_pairs=[]\n",
    "        \n",
    "        \n",
    "        print (\"len(tokenids):{}\".format(len(token_ids))) #equals to the length of original attn matrix len# \n",
    "        print (\"len(tokentokenid2word_mapping):{}\".format(len(tokenid2word_mapping))     )   \n",
    "        print (\"attention_matrix:{}\".format(attention_matrix.shape))\n",
    "        print (\"merged_attention shape:{}\".format (merged_attention.shape))\n",
    "        id2token = {key:value for key,value in enumerate(sentence_mapping)}\n",
    "#############modification starts here ################\n",
    "        \n",
    "        i=0\n",
    "        id2noun_chunk_token=dict()\n",
    "        for key,value in  id2token.items():\n",
    "            if value in noun_chunks:\n",
    "                id2noun_chunk_token[i]=key\n",
    "                i=i+1\n",
    "                \n",
    "#############modification ends here ###########################\n",
    "    #############\n",
    "        def head_tail_pairs_append(head_tail_pairs,sent,id2noun_chunk_token,start_chunk,token2id):\n",
    "#             print ('sent.start:%s, sent.end:%s'%(sent.start,sent.end))\n",
    "            for i1,start1 in enumerate(start_chunk):\n",
    "#                 print ('i1:%s, start1 :%s' %(i1, start1))\n",
    "                if start1 < sent.start:\n",
    "                    continue\n",
    "                if start1 >=sent.end:\n",
    "                    break\n",
    "                for i2,start2 in enumerate(start_chunk):\n",
    "        #             print (\"i2:%s, start2 :%s\"%(i2, start2))\n",
    "                    if start2<start1 :\n",
    "                        continue\n",
    "                    if start2>= sent.end: \n",
    "                        break\n",
    "                    if start1!=start2:\n",
    "        #                 print (\"succeed, start1 :%s,start2:%s\"%(noun_chunks[i1],noun_chunks[i2]))\n",
    "#                         print ('sc')\n",
    "                        head_tail_pairs.append((id2noun_chunk_token[i1],id2noun_chunk_token[i2]))\n",
    "            return head_tail_pairs\n",
    "        for sent in nlp(sentence).sents:\n",
    "            tail_head_pairs = head_tail_pairs_append(tail_head_pairs,sent,id2noun_chunk_token,start_chunk,token2id)\n",
    "            \n",
    "            \n",
    "            #######################################       \n",
    "\n",
    "#should starts from here\n",
    "# add'same sentence ' limit here\n",
    "# first, the pair is limited in the same sentence, 2nd, the funciton is preventing meaningless iteration beyond same sentence.         \n",
    "     \n",
    "#         for sent in nlp(sentence).sents:\n",
    "#             for (head,start_number) in zip(noun_chunks,start_chunk):\n",
    "#                 if start_number < sent.end_number:\n",
    "#                     for (tail,end_number) in zip(noun_chunks,end_chunk):\n",
    "#                         if end_number > sent.end_number:\n",
    "#                             continue\n",
    "#                         if head != tail:\n",
    "#                             tail_head_pairs.append((token2id[head], token2id[tail]))\n",
    "\n",
    "##should ends here\n",
    "                    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        black_list_relation = set([ token2id[n]  for n in noun_chunks ])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        all_relation_pairs = []\n",
    "#         id2token = { value: key for key, value in token2id.items()}\n",
    "        #### modification starts below,\n",
    "        ### id2token = { value: key for key, value in token2id.items()} --->id2token = {key:value for key,value in enumerate(sentence_mapping)}\n",
    "        ### because in token2id dictionary, there is no duplicate tokens, so it won't be reflect the positions with these tokens\n",
    "        ### but the later algorithm needs all positions \n",
    "        id2token = {key:value for key,value in enumerate(sentence_mapping)}\n",
    "        ### modication ends here \n",
    "        print (\"tail_head_pairs:{}\".format(tail_head_pairs)) \n",
    "        print (\"id2token:{}\".format(id2token))\n",
    "        print (\"len_sentence_mapping:{}\".format(len(sentence_mapping)))\n",
    "# '''\n",
    "#         the following is a sample of the result\n",
    "#         noun_chunks:['A highly fire-resistant and environmentally-friendly panel', '2 mm to 28 mm', 'a blending', 'magnesium compounds',\n",
    "#         'sodium silicate', \n",
    "#         'kaolin', 'fillers', 'additives', 'the core materials', '4 layers', 'fire-resistant glass fiber meshes', 'fabrics', \n",
    "#         'a proprietary ITC process', 'the chemical reactions', 'the ingredients', 'sufficient heat', 'external supply', 'energy',\n",
    "#         'the panels', '24 hours', '10 days', 'The use', 'waste materials', 'energy-saving curing system', 'no gas emission manufacturing process', \n",
    "#         'this panel', \"the world's highest-rated fire resistance\", '5 hours', 'high flexural strength', 'low density', 'durability', \n",
    "#         'effective water-resistance']\n",
    "#         start_chunk:[0, 11, 20, 23, 26, 29, 31, 34, 37, 43, 46, 53, 56, 62, 66, 70, 73, 76, 78, 85, 89, 92, 95, 98, 104, 112, 121, 130, 133, 137, 140, 142]\n",
    "# end_chunk:[10, 16, 22, 25, 28, 30, 32, 35, 40, 45, 52, 54, 60, 65, 68, 72, 75, 77, 80, 87, 91, 94, 97, 103, 109, 114, 129, 132, 136, 139, 141, 146]\n",
    "\n",
    "#         end_chunk:[10, 16, 22, 25, 28, 30, 32, 35, 40, 45, 52, 54, 60, 65, 68, 72, 75, 77, 80, 87, 91, 94, 97, 103, 109, 114, 129, 132, 136, 139, 141, 146]\n",
    "#         token2id:{'A highly fire-resistant and environmentally-friendly panel': 0, 'of': 56, '2 mm to 28 mm': 2, 'may': 35, 'be': 35, 'manufactured': 5, \n",
    "# 'by': 19, 'a blending': 7, 'magnesium compounds': 8, ',': 59, 'sodium silicate': 10, 'kaolin': 11, 'fillers': 12, 'and': 60, 'additives': 14, 'to': 46, \n",
    "# 'form': 16, 'the core materials': 17, 'reinforced': 18, '4 layers': 19, 'fire-resistant glass fiber meshes': 20, 'fabrics': 21, '.': 61, 'Using': 23,\n",
    "# 'a proprietary ITC process': 24, 'that': 25, 'accelerates': 26, 'the chemical reactions': 27, 'the ingredients': 28, 'generate': 29, 'sufficient heat': 30, \n",
    "# 'without': 31, 'external supply': 32, 'energy': 33, 'the panels': 34, 'completely': 35, 'cured': 36, 'within': 37, '24 hours': 38, 'instead': 39, '10 days': 40,\n",
    "# 'The use': 41, 'waste materials': 42, 'energy-saving curing system': 43, 'no gas emission manufacturing process': 44, 'combined': 45, 'make': 46, 'this panel':\n",
    "# 47, 'an': 48, 'eco': 49, '-': 50, 'friendly': 51, 'product': 52, 'which': 53, 'offers': 54, \"the world's highest-rated fire resistance\": 55, '5 hours': 56,\n",
    "# 'high flexural strength': 57, 'low density': 58, 'durability': 59, 'effective water-resistance': 60}\n",
    "#         token_ids:[138, 3023, 1783, 118, 13676, 1105, 4801, 1193, 118, 4931, 5962, 1104, 123, 2608, 1106, 1743, 2608]\n",
    "#         tokenid2word_mapping:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 56, 2, 2, 2, 2, 2]\n",
    "#         subtoken_ids:[123, 2608, 1106, 1743, 2608]\n",
    "#         '''\n",
    "\n",
    "        \n",
    "        #####\n",
    "\n",
    "#             for triplets in parse_sentence(sent.text,tokenizer,encoder,nlp,use_cuda=False):\n",
    "#                 print (triplets)\n",
    "#                 valid_triplets.append (triplets)\n",
    "#                 if len(valid_triplets) > 0:\n",
    "#                     # Map\n",
    "#                     mapped_triplets = []\n",
    "#                 for triplet in valid_triplets:\n",
    "#                     head = triplet['h']\n",
    "#                     tail = triplet['t']\n",
    "#                     relations = triplet['r']\n",
    "#                     conf = triplet['c']     \n",
    "#                     if conf < threshold:\n",
    "#                         continue\n",
    "#                     mapped_triplet = Map(head, relations, tail)\n",
    "#                     if 'h' in mapped_triplet:\n",
    "#                         mapped_triplet['c'] = conf\n",
    "#                         mapped_triplets.append(mapped_triplet)\n",
    "#                 output = { 'line': idx, 'tri': deduplication(mapped_triplets) }\n",
    "#                 print (output)\n",
    "#                 if include_sentence:\n",
    "#                     output['sent'] = sentence\n",
    "#                 if len(output['tri']) > 0:\n",
    "#                     g.write(json.dumps( output )+'\\n')\n",
    "        break\n",
    "        ### inputs: A highly fire-resistant and environmentally-friendly panel of 2 mm to 28 mm may be manufactured by a \n",
    "        ##blending of magnesium compounds, sodium silicate, kaolin, fillers, and additives to form the core materials, reinforced \n",
    "        ###by 4 layers of fire-resistant ....\n",
    "        ### outputs: \n",
    "        ### 1, outputs:\n",
    "        ### 2, token2idword_mapping:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 56, \n",
    "        ### 2, 2, 2, 2, 2, 35, 35, 5, 19, 7, 7, 56, 8, 8, 8, 59, 10, 10, 10, 10, 59, 11, 11, 59, 12, 12, 59, 60, 14, 14, 14, 46, 16, 17, 17, 17, 59, 18, 19, 19, 19, 56, 20, 20, 20, \n",
    "        ### 20, 20, 20, 20, 60, 21, 21, 61, 23, 24, 24, 24, 24, 24, 25, 26, 26, 27, 27, 27, 56, 28, 28, 46, 29, 30, 30, 31, 32, 32, 56, 33, 59, 34, 34, 35, 35, 35, 36, 37, 38, 38, \n",
    "        ### 39, 56, 40, 40, 61, 41, 41, 56, 42, 42, 59, 43, 43, 43, 43, 43, 43, 60, 44, 44, 44, 44, 44, 45, 46, 46, 47, 47, 48, 49, 49, 50, 51, 52, 53, 54, 55, 55, 55, 55, 55, 55,\n",
    "        ### 55, 55, 55, 56, 56, 56, 59, 57, 57, 57, 57, 57, 59, 58, 58, 59, 59, 59, 59, 60, 60, 60, 60, 60, 61]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3)\n",
      "(0, 3)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-04acd0551211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtail_head_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mcandidate_facts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBFS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenid2word_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblack_list_relation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/patent/patent_copy2/utils.py\u001b[0m in \u001b[0;36mBFS\u001b[0;34m(s, end, graph, id2token, max_size, black_list_relation)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mcandidate_facts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcum_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mcandidate_facts\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcandidate_facts_relation_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_facts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;31m#     candidate_facts = sorted(candidate_facts, key=lambda x: x[1], reverse=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/patent/patent_copy2/utils.py\u001b[0m in \u001b[0;36mcandidate_facts_relation_filter\u001b[0;34m(candidate_facts, id2token)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#         print (id2token[head].lower() not in invalid_relations_set )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#         print (id2token[tail].lower() not in invalid_relations_set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrelations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_single_relation_validity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrelations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minvalid_relations_set\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minvalid_relations_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mfiltered_candidate_facts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_fact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#     print ('filterd_c_f{} '.format(filtered_candidate_facts))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from utils import BFS\n",
    "def bfs(args):\n",
    "    s, end, graph, max_size, black_list_relation = args\n",
    "    return BFS(s, end, graph, max_size, black_list_relation)\n",
    "def global_initializer(nlp_object):\n",
    "    global spacy_nlp\n",
    "    spacy_nlp = nlp_object\n",
    "def check_relations_validity(relations):\n",
    "    for rel in relations:\n",
    "        if rel.lower() in invalid_relations_set or rel.isnumeric():\n",
    "            return False\n",
    "    return True\n",
    "def filter_relation_sets(params):\n",
    "    triplet, id2token = params\n",
    "\n",
    "    triplet_idx = triplet[0]\n",
    "    confidence = triplet[1]\n",
    "    head, tail = triplet_idx[0], triplet_idx[-1]\n",
    "    if head in id2token and tail in id2token:\n",
    "        head = id2token[head]\n",
    "        tail = id2token[tail]\n",
    "        relations = [ spacy_nlp(id2token[idx])[0].lemma_  for idx in triplet_idx[1:-1] if idx in id2token ]\n",
    "        if len(relations) > 0 and check_relations_validity(relations) and head.lower() not in invalid_relations_set and tail.lower() not in invalid_relations_set:\n",
    "            return {'h': head, 't': tail, 'r': relations, 'c': confidence }\n",
    "    return {}\n",
    "\n",
    "\n",
    "## candidate_facts_filter### end\n",
    "print (tail_head_pairs[0])\n",
    "for pair in tail_head_pairs:    \n",
    "    print (pair)\n",
    "    candidate_facts = BFS(pair[0],pair[1],attn_graph,max(tokenid2word_mapping),black_list_relation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with Pool(10) as pool:\n",
    "#     params = [  ( pair[0], pair[1], attn_graph, max(tokenid2word_mapping), black_list_relation, ) for pair in tail_head_pairs]\n",
    "\n",
    "#     for output in pool.imap_unordered(bfs, params):\n",
    "\n",
    "#         if len(output):\n",
    "#             all_relation_pairs += [(output[0], id2token)]\n",
    "## each relation pair include a pair of relation (h,r,t) and the id2token dictionary##\n",
    "## id2token is added because 'filter_relation_sets' need it as the input to assure the word is in token list \n",
    "\n",
    "# print ('all_r_p',all_relation_pairs[0:4])\n",
    "\n",
    "# triplet_text = []\n",
    "# with Pool(10, global_initializer, (nlp,)) as pool:\n",
    "#     for triplet in pool.imap_unordered(filter_relation_sets, all_relation_pairs):\n",
    "#         if len(triplet) > 0:\n",
    "#             triplet_text.append(triplet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             '''\n",
    "#             output[0], [([0, 1, 2], 0.0370228560641408)]\n",
    "# all_relation_pairs[0] \n",
    "# [(([0, 1, 2], 0.0370228560641408), {0: 'A highly fire-resistant and environmentally-friendly panel',\n",
    "# 76: 'of', 2: '2 mm to 28 mm', 47: 'may', 48: 'be', 5: 'manufactured', 24: 'by', 7: 'a blending', 9: 'magnesium compounds', 82: ',', \n",
    "# 11: 'sodium silicate', 13: 'kaolin', 15: 'fillers', 84: 'and', 18: 'additives', 65: 'to', 20: 'form', 21: 'the core materials', 23: \n",
    "# 'reinforced', 25: '4 layers', 27: 'fire-resistant glass fiber meshes', 29: 'fabrics', 86: '.', 31: 'Using', 32: 'a proprietary ITC process',\n",
    "# 33: 'that', 34: 'accelerates', 35: 'the chemical reactions', 37: 'the ingredients', 39: 'generate', 40: 'sufficient heat', 41: 'without', \n",
    "# 42: 'external supply', 44: 'energy', 46: 'the panels', 49: 'completely', 50: 'cured', 51: 'within', 52: '24 hours', 53: 'instead', \n",
    "# 55: '10 days', 57: 'The use', 59: 'waste materials', 61: 'energy-saving curing system', 63: 'no gas emission manufacturing process',\n",
    "# 64: 'combined', 66: 'make', 67: 'this panel', 68: 'an', 69: 'eco', 70: '-', 71: 'friendly', 72: 'product', 73: 'which', 74: 'offers', \n",
    "# 75: \"the world's highest-rated fire resistance\", 77: '5 hours', 79: 'high flexural strength', 81: 'low density', 83: 'durability', \n",
    "# 85: 'effective water-resistance'})]\n",
    "#             '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_s=tail_head_pairs[1][0]\n",
    "pass_end=tail_head_pairs[1][1]\n",
    "pass_parm_attn_graph=attn_graph\n",
    "pass_parm_max=max(tokenid2word_mapping)\n",
    "pass_parm_black_list_relation=black_list_relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_single_relation_validity(relations):\n",
    "    if relations.lower() in invalid_relations_set or relations.isnumeric():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def candidate_facts_relation_filter(candidate_facts):\n",
    "    filtered_candidate_facts=[]\n",
    "    for candidate_fact in candidate_facts:\n",
    "        head=candidate_fact[0][0]\n",
    "        tail=candidate_fact[0][2]\n",
    "        relations=candidate_fact[0][1]\n",
    "#         print ('%s %s %s'%(id2token[head],id2token[tail],id2token[relations]))\n",
    "#         print ( len(id2token[relations]) > 0 )\n",
    "#         print (id2token[relations])\n",
    "#         print (check_single_relation_validity(id2token[relations]) )\n",
    "#         print (id2token[head].lower() not in invalid_relations_set )\n",
    "#         print (id2token[tail].lower() not in invalid_relations_set)\n",
    "        if len(id2token[relations]) > 0 and check_single_relation_validity(id2token[relations]) and id2token[head].lower() not in invalid_relations_set and id2token[tail].lower() not in invalid_relations_set:\n",
    "            filtered_candidate_facts.append(candidate_fact)\n",
    "\n",
    "    print (filtered_candidate_facts)\n",
    "    if len(filtered_candidate_facts)>0:\n",
    "        filtered_candidate_facts = sorted(filtered_candidate_facts, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        filtered_candidate_facts = filtered_candidate_facts[0]\n",
    "        return (filtered_candidate_facts)\n",
    "    if len(filtered_candidate_facts)==0:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# print (attn_graph)\n",
    "from utils import BFS\n",
    "# result =BFS(pass_s,pass_end,pass_parm_attn_graph,pass_parm_max,pass_parm_black_list_relation)\n",
    "result= [([0, 1, 5], 0.044546898920089006)]\n",
    "candidate_facts = candidate_facts_relation_filter(result)\n",
    "print (candidate_facts)\n",
    "\n",
    "\n",
    "print (\"id2token:{}\".format(id2token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def global_initializer(nlp_object):\n",
    "    global spacy_nlp\n",
    "    spacy_nlp = nlp_object\n",
    "def filter_relation_sets(params):\n",
    "    triplet, id2token = params\n",
    "    global spacy\n",
    "    spacy_nlp= en_core_web_md.load()\n",
    "    triplet_idx = triplet[0]\n",
    "    confidence = triplet[1]\n",
    "    head, tail = triplet_idx[0], triplet_idx[-1]\n",
    "    if head in id2token and tail in id2token:\n",
    "        head = id2token[head]\n",
    "        tail = id2token[tail]\n",
    "        relations = [ spacy_nlp(id2token[idx])[0].lemma_  for idx in triplet_idx[1:-1] if idx in id2token ]\n",
    "        if len(relations) > 0 and check_relations_validity(relations) and head.lower() not in invalid_relations_set and tail.lower() not in invalid_relations_set:\n",
    "            return {'h': head, 't': tail, 'r': relations, 'c': confidence }\n",
    "    return {}\n",
    "all_relation_pairs= [ (perresult, id2token) for perresult in result]\n",
    "from mapper import deduplication\n",
    "triplet_text = []\n",
    "global_initializer(nlp)\n",
    "triplet_text = []\n",
    "global spacy_nlp\n",
    "f_r= [filter_relation_sets(result)for result in all_relation_pairs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relate\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "relations = nlp('relates to')[0].lemma_\n",
    "print (relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relates to\n"
     ]
    }
   ],
   "source": [
    "relations = nlp('relates to')\n",
    "print (relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got spacy.tokens.doc.Doc)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8d38a922c3be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/patent/language-models-are-knowledge-graphs-pytorch-main/env/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             )\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/patent/language-models-are-knowledge-graphs-pytorch-main/env/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_format_docs_and_golds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got spacy.tokens.doc.Doc)"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAMA_v1",
   "language": "python",
   "name": "lama_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
